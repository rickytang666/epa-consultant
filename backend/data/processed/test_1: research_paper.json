{
  "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
  "filename": "test_1: research_paper.pdf",
  "document_summary": "HF-VTON is a high-fidelity virtual-try-on framework that enforces geometric, semantic, and fine-detail consistency via three cooperating modules: an Appearance-Preserving Warp Alignment Module (APWAM) for multi-scale, deformable flow warping, a Semantic Representation and Comprehension Module (SRCM) that provides fine-grained garment attributes, text generation, and a new multi-pose SAMP-VTONS dataset, and a Multimodal Prior-Guided Appearance Generation Module (MPAGM) that fuses geometric priors with visual and textual embeddings using diffusion-based generation. The method improves handling of non-rigid deformations, preserves wrinkles and textures, and aligns garments to body parts more precisely than prior GAN- and U-Net-based approaches. Experiments on VITON\u2011HD and SAMP\u2011VTONS show state-of-the-art quantitative and qualitative results (better SSIM/LPIPS/FID/KID and robustness across poses), with ablations confirming the complementary benefits of each module. Remaining limitations include extreme poses, very intricate textures, reliance on large labeled data and pose estimates, and high computational cost, which the authors suggest addressing via robustness and efficiency work.",
  "section_summaries": {
    "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment > II. RELATED WORK > A. GANs-Based Virtual Try-On": "This subsection reviews GAN-based virtual try-on methods, highlighting GANs\u2019 role in producing realistic, high-resolution garment images and enabling multimodal control. It cites key variants and systems\u2014StyleGAN for high-fidelity editing, CAGAN for conditional image analogies, VTNFP\u2019s deformation-segmentation-fusion pipeline, LA-VITON\u2019s geometric matching with occlusion handling, ACGPN\u2019s use of semantic layouts and TPS stability, and GP-VTON\u2019s combination of local/global deformation and gradient truncation to preserve semantics and texture.",
    "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment > II. RELATED WORK > B. U-Net-Based Virtual Try-On": "This subsection reviews U-Net\u2013based virtual try-on methods: VITON introduced a garment-independent human representation and U-Net synthesis, CP-VTON adds a geometric matching module (TPS) and composite mask to reduce artifacts, while Versatile-VTON and SPATT extend transformation and texture-preservation techniques for diverse garments and poses. It notes U-Net strengths (fine detail preservation and alignment) and limitations (sensitivity to input quality and difficulty modeling complex deformations), and highlights a trend toward diffusion models for more robust, high-resolution garment generation.",
    "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment > II. RELATED WORK > C. Diffusion Model-Based Virtual Try-On": "This section reviews diffusion-model based VTON approaches that outperform GANs/U-Net in image quality, training stability, and detail preservation\u2014especially for complex garment deformations and diverse poses. It cites key advances (DDPM, LDM) and recent architectures (TryOnDiffusion, DCI-VTON, StableVITON, OOTDiffusion, StableGarment) that use latent spaces, VAE encoders, ControlNet, and cross-attention to better preserve textures and learn semantic correspondences. The authors highlight a trend toward fine-grained control and multimodal conditioning, which HF-VTON addresses with a multimodal prior-guided generation module.",
    "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment > II. RELATED WORK > D. Challenges in Existing Virtual Try-On Methods": "Existing virtual try-on methods struggle with consistency under non-rigid garment deformations and diverse poses, relying heavily on high-quality inputs and precise geometric alignment. They also face challenges in multimodal data handling, garment-body alignment, and preserving fine-grained garment details, motivating the HF-VTON framework that integrates modules to unify geometric, semantic, and detail consistency.",
    "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment > III. METHODOLOGY > A. Overview of the HF-VTON Framework": "This section overviews HF-VTON, a VTON framework that enforces consistent geometric alignment, semantic understanding, and fine-detail preservation via three cooperating modules: APWAM (warp alignment), SRCM (semantic representation), and MPAGM (prior-guided appearance generation). Together they enable precise garment fitting and photorealistic image synthesis that is robust across diverse poses and garment types (see Figure 2).",
    "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment > III. METHODOLOGY > B. Appearance-Preserving Warp Alignment Module (APWAM)": "APWAM aligns garments to person images while preserving appearance by (1) MRE: a pyramid PFEN that extracts and fuses multi-scale low- and high-level features from garment, person image, 3D keypoints and segmentation into F_final (weights \u03b1,\u03b2) and (2) DFEN: N-stage cascaded deformable convolutions that learn offsets \u0394p_n to refine appearance flow progressively (p_i = p_{i-1} \u2295 \u0394flow_i), enabling accurate handling of wrinkles, stretching and other non-rigid deformations.",
    "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment > III. METHODOLOGY > C. Semantic Representation and Comprehension Module (SRCM)": "The SRCM enhances semantic understanding for virtual try-on via three components: SAS extends garment attribute taxonomy with fine-grained categories (fit, pattern, color, neckline, collar, sleeve length, shirt length) for precise descriptions; DMTG hybridizes commercial, open-source, and production text models to balance accuracy, speed, and safety when generating garment captions; and SAMP-VTONS provides a semantically aligned multi-pose dataset (21,104 train / 7,487 test) containing garment images, masks, attribute texts, pose estimates (DensePose/OpenPose), human part segmentation, and identity-agnostic preprocessing to support multi-pose training and evaluation.",
    "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment > III. METHODOLOGY > D. Multimodal Prior-Guided Appearance Generation Module (MPAGM)": "MPAGM fuses geometric priors and multimodal semantic/visual information through three submodules\u2014Geometric-Conditioned Multimodal Fusion (GCMF), Cross-Modal Semantic and Visual Fusion (CSVF), and Dual-Conditioned Guidance Appearance Generation (DGAG)\u2014to improve virtual try-on realism. It encodes warped garment and identity-agnostic human features with a Stable Diffusion encoder, concatenates those with mask, pose and noise to form spatially consistent geometric priors, aligns text and visual embeddings in a joint space, and trains a diffusion denoising network with dual image\u2013text conditioning (L_DM) to generate high-fidelity, semantically consistent garment renderings.",
    "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment > IV. EXPERIMENTS > A. Experimental Setup": "Describes experimental setup using VITON\u2011HD (11,647 train / 2,032 test, 384\u00d7512 after resizing; textual descriptions generated via the DMTG module) and the newly extended SAMP\u2011VTONS (21,104 train / 7,487 test, 384\u00d7512, multi\u2011pose and paired text), the evaluation metrics (SSIM, LPIPS, FID, KID), and implementation environment (A800 GPU, Python 3.8, PyTorch 2.0.1, CUDA 11.8) with core modules APWAM, CSVF, and DGAG; quantitative results table shows HF\u2011VTON achieves the best or near\u2011best scores compared to prior methods.",
    "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment > IV. EXPERIMENTS > B. Comparisons with State-of-the-art Methods": "HF-VTON outperforms recent virtual-try-on methods on VITON-HD and SAMP-VTONS across paired and unpaired settings, achieving state-of-the-art scores (e.g., VITON-HD paired FID 5.51, KID 0.024; SAMP-VTONS single-pose SSIM 0.892, LPIPS 0.043) and notable relative gains over LaDI-VTON and MV-VTON. Qualitatively, HF-VTON provides more precise garment-body alignment, better handling of complex deformations, and superior texture/detail preservation.",
    "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment > IV. EXPERIMENTS > C. Robustness Analysis": "On the SAMP-VTONS multi-pose benchmark (paired and unpaired), HF-VTON shows superior robustness: a tiny LPIPS change between single- and multi-pose (\u0394=0.0015 vs. 0.0095 and 0.0175 for DCI-/MV-VTON) and a 31.7% smaller SSIM degradation versus DCI-VTON, indicating strong texture and structural consistency. Qualitative results show HF-VTON achieves more precise geometric alignment (especially shoulders/upper arms) and better preserves garment details and deformations (wrinkles/stretching) than competing methods.",
    "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment > IV. EXPERIMENTS > D. Ablation Study": "Ablation studies on VITON-HD and SAMP-VTONS show APWAM with deformable-flow DFEN yields the largest gains in structural and perceptual metrics (e.g., SSIM 0.849/0.872, LPIPS 0.066/0.083, FID 6.93/5.3, KID 0.14 on SAMP\u2011VTONS) and markedly improves garment-person alignment and wrinkle/detail preservation. Adding SRCM with the proposed text representation (SAS+DMTG) further boosts all quantitative metrics and texture quality versus raw text guidance or no text. MPAGM provides additional reductions in distribution discrepancy and image distortion (improving FID beyond APWAM- or SRCM-only setups), and qualitative comparisons confirm fewer misalignments and less texture distortion, especially around shoulders, chest, hems, and logos.",
    "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment > V. DISCUSSION > A. Insights on Consistency": "The section explains how HF-VTON achieves geometric, semantic, and fine-grained consistency through three modules: APWAM (multi-scale warping that preserves wrinkles and material detail), SRCM (extended semantic garment representations using multi-pose data for precise body-garment alignment), and MPAGM (combines geometric priors with semantic/visual cues for realistic image synthesis). Together these components improve accurate garment fitting and consistent appearance across diverse poses and garment types.",
    "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment > V. DISCUSSION > B. Limitations and Future Directions": "The section highlights limitations in handling extreme poses, intricate textures and non-rigid garment deformations, dependence on large labeled datasets and accurate pose estimates, and high computational cost for real-time applications. It recommends future work on robustness (unsupervised/semi-supervised learning), runtime optimization (model compression, distillation, hardware acceleration), expanding to other garment types, and leveraging additional sensor modalities (depth/multi-view).",
    "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment > I. INTRODUCTION": "The introduction frames VTON\u2019s importance and reviews existing GAN-, U-Net-, and diffusion-based methods and their limitations (structural misalignment, limited non-rigid deformation modeling, instability). It presents HF-VTON, a framework that addresses consistency in geometric alignment, semantic understanding, and fine-grained detail preservation via three novel modules, and reports superior results on VITON-HD and SAMP-VTONS compared to state-of-the-art methods.",
    "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment > II. RELATED WORK": "This related-work section reviews three main families of virtual-try-on methods\u2014GAN-based, U-Net-based, and diffusion-based\u2014highlighting representative systems (e.g., StyleGAN variants, CP-VTON/VTNFP, and recent diffusion approaches like TryOnDiffusion) and core techniques such as TPS geometric matching, semantic-layout conditioning, and texture preservation. It notes a shift toward diffusion models for more stable, high-fidelity, multimodal garment synthesis and identifies key challenges (handling non-rigid deformation, diverse poses, multimodal inputs, and preserving fine-grained garment details) that motivate the HF-VTON design.",
    "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment > VI. CONCLUSION": "The paper proposes HF-VTON, a virtual try-on framework combining an Appearance-Preserving Warp Alignment Module (APWAM), a Semantic Representation and Comprehensive Module (SRCM), and a Multimodal Prior-Guided Appearance Generation Module (MPAGM) to enforce geometric, semantic, and appearance consistency while preserving fine garment details. Experiments show it outperforms prior methods in visual quality, semantic alignment, and fitting accuracy, though it struggles with extreme poses and very complex garments. Future work targets improved robustness, computational efficiency, and support for more garment types.",
    "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment > REFERENCES": "This section provides the REFERENCES cited by HF\u2011VTON, covering prior work on high-resolution virtual try-on (e.g., VITON\u2011HD, DressCode, Versatile\u2011VTON), image quality metrics (SSIM), StyleGAN inversion and related generative/image\u2011processing methods, texture/flow transformers and diffusion-based approaches for garment transfer. It concludes with an author biography for Qi Dong, a master\u2019s student at the Communication University of China whose research interests are computer vision and digital image processing.",
    "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment": "This section introduces HF-VTON, a high-fidelity virtual try-on framework combining an Appearance-Preserving Warp Alignment Module (APWAM), a Semantic Representation and Comprehension Module (SRCM), and a Multimodal Prior-Guided Appearance Generation Module (MPAGM) to enforce geometric, semantic, and fine-detail consistency. It presents the new SAMP\u2011VTONS multi-pose/text-annotated dataset, quantitative and qualitative gains over state-of-the-art methods on VITON\u2011HD and SAMP\u2011VTONS (better SSIM/LPIPS/FID/KID and robustness across poses), ablation studies showing module contributions, and discusses limitations (extreme poses, complex textures, computational cost) and future directions."
  },
  "header_tree": {
    "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment": {
      "I. INTRODUCTION": {},
      "II. RELATED WORK": {
        "A. GANs-Based Virtual Try-On": {},
        "B. U-Net-Based Virtual Try-On": {},
        "C. Diffusion Model-Based Virtual Try-On": {},
        "D. Challenges in Existing Virtual Try-On Methods": {}
      },
      "III. METHODOLOGY": {
        "A. Overview of the HF-VTON Framework": {},
        "B. Appearance-Preserving Warp Alignment Module (APWAM)": {},
        "C. Semantic Representation and Comprehension Module (SRCM)": {},
        "D. Multimodal Prior-Guided Appearance Generation Module (MPAGM)": {}
      },
      "IV. EXPERIMENTS": {
        "A. Experimental Setup": {},
        "B. Comparisons with State-of-the-art Methods": {},
        "C. Robustness Analysis": {},
        "D. Ablation Study": {}
      },
      "V. DISCUSSION": {
        "A. Insights on Consistency": {},
        "B. Limitations and Future Directions": {}
      },
      "VI. CONCLUSION": {},
      "REFERENCES": {}
    }
  },
  "costs": {
    "header_correction": 0.0,
    "skeleton_summaries": 0.019091000000000004,
    "total": 0.020314500000000003,
    "document_summary": 0.0012235
  },
  "chunks": [
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "Ming Meng<sup>1,\\*</sup> Qi Dong<sup>1,\\*</sup> Jiajie Li<sup>1</sup> Zhe Zhu<sup>2</sup> Xingyu Wang<sup>3</sup> Zhaoxin Fan<sup>3,4,\u2020</sup>  \n Wei Zhao<sup>1,\u2020</sup> Wenjun Wu<sup>3</sup>\n\n<sup>1</sup>School of Data Science and Media Intelligence, Communication University of China, Beijing, China\n\n<sup>2</sup>Samsung Research America, USA\n\n<sup>3</sup>Beijing Advanced Innovation Center for Future Blockchain and Privacy Computing, School of Artificial Intelligence, Beihang University, Beijing, China\n\n<sup>4</sup>Hangzhou International Innovation Institute, Beihang University, Beijing, China\n\nmengming@cuc.edu.cn, zcdq@cuc.edu.cn, lijiajie@cuc.edu.cn,  \n ajex1988@gmail.com, 20377014@buaa.edu.cn, zhaoxin@buaa.edu.cn,  \n zhao\\_wei@cuc.edu.cn, wwj09315@buaa.edu.cn",
      "chunk_index": "1-0",
      "location": {
        "page_number": 1
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "mengming@cuc.edu.cn, zcdq@cuc.edu.cn, lijiajie@cuc.edu.cn,  \n ajex1988@gmail.com, 20377014@buaa.edu.cn, zhaoxin@buaa.edu.cn,  \n zhao\\_wei@cuc.edu.cn, wwj09315@buaa.edu.cn\n\n![Figure 1: A comparison of virtual try-on methods. The figure shows two rows of images. The top row shows a red and blue FILA t-shirt being tried on by a person in various poses. The bottom row shows a dark red parka with a fur collar being tried on by a person in various poses. Columns are labeled: Garment, Person, PF-AFN, LaDI-VTON, DM-VTON, MV-VTON, and HF-VTON (Ours). Red boxes highlight errors in the existing methods (PF-AFN, LaDI-VTON, DM-VTON, MV-VTON), such as logo distortion and misalignment. Green boxes highlight the superior results of the proposed HF-VTON method.](6ed175c791b5e156d9c98a8dbcc3318c_img.jpg)",
      "chunk_index": "1-1",
      "location": {
        "page_number": 1
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "Figure 1: A comparison of virtual try-on methods. The figure shows two rows of images. The top row shows a red and blue FILA t-shirt being tried on by a person in various poses. The bottom row shows a dark red parka with a fur collar being tried on by a person in various poses. Columns are labeled: Garment, Person, PF-AFN, LaDI-VTON, DM-VTON, MV-VTON, and HF-VTON (Ours). Red boxes highlight errors in the existing methods (PF-AFN, LaDI-VTON, DM-VTON, MV-VTON), such as logo distortion and misalignment. Green boxes highlight the superior results of the proposed HF-VTON method.\n\nFig. 1. Our method (HF-VTON) outperforms existing state-of-the-art methods (e.g., PF-AFN [14], LaDI-VTON [23], DM-VTON [50], MV-VTON [34]) on two datasets: VITON-HD [1](single pose) and SAMP-VTONS (Pose 1 and Pose 2). Red boxes highlight errors, such as issues with logo details, geometric alignment, and texture fidelity, while green boxes emphasize the superior results produced by our approach.",
      "chunk_index": "1-2",
      "location": {
        "page_number": 1
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "**Abstract**\u2014Virtual try-on technology has become increasingly important in the fashion and retail industries, enabling the generation of high-fidelity garment images that adapt seamlessly to target human models. While existing methods have achieved notable progress, they still face significant challenges in maintaining consistency across different poses. Specifically, geometric distortions lead to a lack of spatial consistency, mismatches in garment structure and texture across poses result in semantic inconsistency, and the loss or distortion of fine-grained details diminishes visual fidelity. To address these challenges, we propose HF-VTON, a novel framework that ensures high-fidelity virtual try-on performance across diverse poses. HF-VTON consists of three key modules: (1) the Appearance-Preserving Warp Alignment Module (APWAM), which aligns garments to human poses, addressing geometric deformations and ensuring spatial consistency; (2) the Semantic Representation and",
      "chunk_index": "1-3",
      "location": {
        "page_number": 1
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "the Appearance-Preserving Warp Alignment Module (APWAM), which aligns garments to human poses, addressing geometric deformations and ensuring spatial consistency; (2) the Semantic Representation and Comprehension Module (SRCM), which captures fine-grained garment attributes and multi-pose data to enhance semantic representation, maintaining structural, textural, and pattern consistency; and (3) the Multimodal Prior-Guided Appearance Generation Module (MPAGM), which integrates multimodal features and prior knowledge from pre-trained models to optimize appearance generation, ensuring both semantic and geometric consistency.",
      "chunk_index": "1-4",
      "location": {
        "page_number": 1
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "Additionally, to overcome data limitations in existing benchmarks, we introduce the SAMP-VTONS dataset, featuring multi-pose pairs and rich textual annotations for a more comprehensive evaluation. Experimental results demonstrate that HF-VTON outperforms state-of-the-art methods on both VITON-HD and SAMP-VTONS, excelling in visual fidelity, semantic consistency, and detail preservation. The code and dataset will be made publicly available upon acceptance: <https://github.com/mmiph/HF-VTON/>.\n\n**Index Terms**\u2014Virtual Try-On, Consistency Alignment, High-Fidelity, Multimodal Feature Fusion",
      "chunk_index": "1-5",
      "location": {
        "page_number": 1
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "VIRTUAL try-on (VTON) has established itself as a cornerstone task within the realms of computer vision and virtual reality, driven by its profound potential to reshape industries such as e-commerce, fashion retail, and virtual human modeling. By seamlessly synthesizing garments onto human bodies in a highly realistic and natural manner\u2014while rigorously preserving the fine-grained details of body pose, shape, and appearance\u2014VTON addresses a fundamental challenge in personalized digital fashion visualization. This task not\n\n\\*These authors contributed equally to this work.\n\n\u2020Corresponding author.",
      "chunk_index": 2,
      "location": {
        "page_number": 1
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "I. INTRODUCTION"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "I. INTRODUCTION"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "only promises to redefine the online shopping experience by offering unprecedented levels of interactivity and immersion but also serves as a benchmark for advancing core computer vision problems, including human parsing, pose estimation, and image synthesis. As the demand for intelligent, customer-centric virtual solutions accelerates, VTON stands at the forefront of research, unlocking new opportunities for practical deployment and fueling innovation in consumer-centric AI systems.",
      "chunk_index": "3-0",
      "location": {
        "page_number": 2
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "I. INTRODUCTION"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "I. INTRODUCTION"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "Existing virtual try-on methods can be broadly categorized into three main paradigms, each employing distinct modeling approaches to tackle this challenging task. GAN-based methods (e.g., CP-VTON [35]) exploit adversarial training between a generator and a discriminator to learn the mapping of garment regions, thereby enhancing the realism of the generated images. While these methods have achieved notable success in improving visual fidelity, they are prone to mode collapse and often exhibit structural misalignments when dealing with complex garment designs or diverse pose variations. U-Net-based methods (e.g., PF-AFN [14]), on the other hand, utilize encoder-decoder architectures with skip connections to reconstruct images, delivering strong texture preservation and enhanced garment details. However, these methods lack the flexibility to model non-rigid garment deformations, limiting their ability to handle intricate clothing shapes and movements. More recently, diffusion model-based",
      "chunk_index": "3-1",
      "location": {
        "page_number": 2
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "I. INTRODUCTION"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "I. INTRODUCTION"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "However, these methods lack the flexibility to model non-rigid garment deformations, limiting their ability to handle intricate clothing shapes and movements. More recently, diffusion model-based methods (e.g., DM-VTON [50]) have emerged as a powerful alternative, leveraging iterative denoising processes to generate high-fidelity images. This approach has demonstrated state-of-the-art performance in terms of stability and detail retention, setting a new benchmark for VTON tasks. Despite the remarkable achievements of these above mentioned methods in improving visual quality and realism, significant challenges remain, particularly in generating spatially consistent images.",
      "chunk_index": "3-2",
      "location": {
        "page_number": 2
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "I. INTRODUCTION"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "I. INTRODUCTION"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "Specifically, as shown in Figure 1, existing VTON methods face three key consistency challenges: geometric alignment, semantic matching, and fine-grained detail preservation. First, geometric consistency requires garments to align accurately with the human body across diverse poses, yet current methods often fail to handle complex deformations, leading to unnatural distortions [55], [58]. Second, semantic consistency demands stable preservation of garment attributes such as style, color, and texture during pose transitions, but existing approaches frequently result in mismatches that undermine reliability [13], [23]. Lastly, fine-grained consistency focuses on retaining intricate garment details like wrinkles and textures, which are often lost or distorted under pose variations, reducing realism and user engagement [12], [35]. Though these challenges are typically addressed individually, there is a lack of a unified framework to resolve them simultaneously, which hampers the stability",
      "chunk_index": "3-3",
      "location": {
        "page_number": 2
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "I. INTRODUCTION"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "I. INTRODUCTION"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "and user engagement [12], [35]. Though these challenges are typically addressed individually, there is a lack of a unified framework to resolve them simultaneously, which hampers the stability and quality of cross-pose virtual try-on results. Moreover, the absence of large-scale, accurately annotated cross-modal datasets further exacerbates these challenges, hindering progress in addressing consistency issues comprehensively [11].",
      "chunk_index": "3-4",
      "location": {
        "page_number": 2
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "I. INTRODUCTION"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "I. INTRODUCTION"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "To address the aforementioned consistency challenges, we propose HF-VTON, a high-fidelity virtual try-on framework that systematically tackles geometric, semantic, and fine-grained consistency. The framework integrates multiple modules that work collaboratively to precisely align garments with the human body while preserving fine-grained garment details. Specifically, it includes the Appearance-Preserving Warp Alignment Module (APWAM), which leverages multi-scale appearance representation extraction and deformable",
      "chunk_index": "3-5",
      "location": {
        "page_number": 2
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "I. INTRODUCTION"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "I. INTRODUCTION"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "flow estimation networks to handle non-rigid garment deformations, ensuring geometric consistency across complex poses. The Semantic Representation and Comprehension Module (SRCM) constructs a structured garment attribute framework and utilizes large-scale image understanding models to enhance semantic consistency, ensuring accurate matching between garments and body poses. Finally, the Multimodal Prior-Guided Appearance Generation Module (MPAGM) integrates geometric conditions and multimodal semantic information to optimize fine-grained detail generation, preserving realistic garment features in high-fidelity virtual try-on images.",
      "chunk_index": "3-6",
      "location": {
        "page_number": 2
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "I. INTRODUCTION"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "I. INTRODUCTION"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "We evaluate HF-VTON on two datasets: VITON-HD (single pose) and SAMP-VTONS (cross-pose with Pose 1 and Pose 2). The results show that HF-VTON significantly outperforms state-of-the-art methods in visual fidelity, semantic consistency, and fine-grained detail preservation. Our approach handles complex garment deformations and diverse pose variations with superior accuracy, achieving consistent and realistic virtual try-on results across multiple poses. These findings demonstrate the effectiveness of our framework and its potential for real-world virtual try-on applications. Our contribution can be summarized as:",
      "chunk_index": "3-7",
      "location": {
        "page_number": 2
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "I. INTRODUCTION"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "I. INTRODUCTION"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "- We propose the HF-VTON framework, which systematically addresses the consistency challenges in virtual try-on from three key aspects: geometric alignment, semantic understanding, and fine-grained detail preservation.\n- We design three novel modules to tackle these challenges: the Appearance-Preserving Warp Alignment Module for precise garment-body alignment across complex poses, the Semantic Representation and Comprehension Module for enhancing semantic consistency through a structured Garment Attribute Structure, and the Multimodal Prior-Guided Appearance Generation Module for preserving fine-grained garment details and generating high-fidelity virtual try-on images.\n- We conduct extensive experiments on the VITON-HD and SAMP-VTONS datasets, demonstrating that HF-VTON significantly outperforms state-of-the-art methods in terms of visual fidelity, consistency, and detail preservation, validating its effectiveness for real-world virtual try-on applications.",
      "chunk_index": "3-8",
      "location": {
        "page_number": 2
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "I. INTRODUCTION"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "I. INTRODUCTION"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "The advancement of image-based virtual try-on technology has been a key focus in recent research. As a crucial component for realistic garment fitting and human interaction, it offers valuable insights for applications such as personalized fashion recommendations and virtual fitting rooms. In this context, we discuss the core approaches that are most relevant to our work, including Generative Adversarial Networks (GANs), U-Net architectures, and Diffusion Models, which have significantly contributed to improving the quality of virtual try-on tasks.",
      "chunk_index": 4,
      "location": {
        "page_number": 2
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "II. RELATED WORK"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "II. RELATED WORK"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "Generative Adversarial Networks (GANs) [63] have played a critical role in virtual try-on technology, particularly in enhancing image realism and try-on effectiveness. Through adversarial training between the generator and discriminator, GANs can generate detailed and texture-rich garment images that naturally align with the human body. As a notable variant of GANs, StyleGAN [53] excels in high-fidelity image editing [3] and efficient image compression [4]. In recent years,",
      "chunk_index": 5,
      "location": {
        "page_number": 2
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "II. RELATED WORK"
        },
        {
          "level": "Header 3",
          "name": "A. GANs-Based Virtual Try-On"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "II. RELATED WORK",
        "Header 3": "A. GANs-Based Virtual Try-On"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "StyleGAN has been widely applied to virtual try-on tasks, including cross-domain garment translation, high-resolution image generation, and multimodal conditional control, significantly improving the visual quality of try-on images and laying the foundation for personalized garment recommendations and virtual digital avatar applications. As a milestone work, N. Jetchev et al. introduced the Conditional Analogy Generative Adversarial Network (CAGAN [54]), which, for the first time, learns the relationship between paired images in the training data through adversarial training and deep convolutional networks, generating plausible image pairs that follow learned relationships, even if unseen during training set. Ruiyun Yu et al. proposed the VTNFP [55] model, which adopts a three-stage deformation-segmentation-fusion architecture. This model deforms garments to fit the target pose, predicts body segmentation maps, and fuses the garments with the human image, utilizing GANs to achieve",
      "chunk_index": "6-0",
      "location": {
        "page_number": 3
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "II. RELATED WORK"
        },
        {
          "level": "Header 3",
          "name": "A. GANs-Based Virtual Try-On"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "II. RELATED WORK",
        "Header 3": "A. GANs-Based Virtual Try-On"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "architecture. This model deforms garments to fit the target pose, predicts body segmentation maps, and fuses the garments with the human image, utilizing GANs to achieve fine-grained virtual try-on image synthesis. The same year, Hyug Jae Lee et al. introduced the LA-VITON [56] virtual try-on network, which combines geometric matching and try-on modules, optimizing garment deformation through grid spacing consistency loss and occlusion handling techniques, and seamlessly generating the final try-on image using GANs. Han Yang et al. proposed the ACGPN [57] model, which introduces semantic layout for the first time, combining adaptive content generation and retention schemes, and enhancing thin-plate spline stability with second-order difference constraints to improve the handling of complex garment textures, generating high-quality virtual try-on images through adversarial training. Zhenyu Xie et al. proposed the GP-VTON [58] model, which combines local flow and global analysis",
      "chunk_index": "6-1",
      "location": {
        "page_number": 3
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "II. RELATED WORK"
        },
        {
          "level": "Header 3",
          "name": "A. GANs-Based Virtual Try-On"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "II. RELATED WORK",
        "Header 3": "A. GANs-Based Virtual Try-On"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "complex garment textures, generating high-quality virtual try-on images through adversarial training. Zhenyu Xie et al. proposed the GP-VTON [58] model, which combines local flow and global analysis deformation modules with dynamic gradient truncation strategies, addressing semantic information preservation and texture distortion issues under complex inputs, generating semantically consistent and complete garment images.",
      "chunk_index": "6-2",
      "location": {
        "page_number": 3
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "II. RELATED WORK"
        },
        {
          "level": "Header 3",
          "name": "A. GANs-Based Virtual Try-On"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "II. RELATED WORK",
        "Header 3": "A. GANs-Based Virtual Try-On"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "Despite significant advancements in virtual try-on using GANs, challenges remain in handling complex garment deformations, detail preservation, and diverse poses. Particularly, the quality of generated images is sensitive to imbalanced data distributions, and the hyperparameter tuning process is complex and costly. Consequently, researchers have turned to alternative approaches to overcome these limitations. U-Net-based virtual try-on methods, with their advantages in detail preservation, contextual integration, and local feature capture, have emerged as another important research direction in this field.",
      "chunk_index": "6-3",
      "location": {
        "page_number": 3
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "II. RELATED WORK"
        },
        {
          "level": "Header 3",
          "name": "A. GANs-Based Virtual Try-On"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "II. RELATED WORK",
        "Header 3": "A. GANs-Based Virtual Try-On"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "U-Net [59], introduced by Olaf Ronneberger et al., is initially designed for medical image segmentation. Its distinctive encoder-decoder architecture with skip connections has significantly advanced virtual try-on tasks. U-Net effectively captures fine-grained details, preserves key human features, and handles garment deformation, ensuring that generated images appear both natural and realistic. Xintong Han et al. proposed the VITON model [46], which introduces a garment-independent human representation to improve garment-body alignment. The model uses U-Net to synthesize a reference image conditioned on the target garment and human representation, further refining the coarse result with a refinement network to produce more natural try-on images. In the same year, Bochao Wang et al. introduced CP-VTON [35], building on VITON by incorporating a geometric matching module",
      "chunk_index": "7-0",
      "location": {
        "page_number": 3
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "II. RELATED WORK"
        },
        {
          "level": "Header 3",
          "name": "B. U-Net-Based Virtual Try-On"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "II. RELATED WORK",
        "Header 3": "B. U-Net-Based Virtual Try-On"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "that learns thin-plate spline transformations to adapt garments, followed by a try-on module that generates a composite mask to merge deformed garments with rendered images, reducing boundary artifacts and enhancing realism. As virtual try-on technology evolves, researchers continue to innovate in model diversity and practicality. Jin Hyun-woo et al. proposed Versatile-VTON [10], which combines explicit garment transformation networks and probabilistic models to enable versatile try-on for various garment types. In addition, Liu et al. proposed SPATT [5], which integrates U-Net into a spatial-aware texture transformer framework for high-fidelity garment transfer, demonstrating strong performance in preserving fine-grained garment details and handling pose variations.",
      "chunk_index": "7-1",
      "location": {
        "page_number": 3
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "II. RELATED WORK"
        },
        {
          "level": "Header 3",
          "name": "B. U-Net-Based Virtual Try-On"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "II. RELATED WORK",
        "Header 3": "B. U-Net-Based Virtual Try-On"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "While U-Net-based models excel at preserving details and alignment, their dependence on high-quality input images and difficulty in modeling complex garment deformations limit their robustness. To address these challenges, researchers have turned to diffusion models, which demonstrate greater robustness and stability in generating high-resolution images and preserving details, particularly in handling complex garment deformations and varied poses.",
      "chunk_index": "7-2",
      "location": {
        "page_number": 3
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "II. RELATED WORK"
        },
        {
          "level": "Header 3",
          "name": "B. U-Net-Based Virtual Try-On"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "II. RELATED WORK",
        "Header 3": "B. U-Net-Based Virtual Try-On"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "In virtual try-on technology, diffusion models have emerged as a powerful alternative to GANs and U-Net, demonstrating exceptional performance in generating high-quality images, preserving details, and ensuring training stability. Their generalization ability has also been demonstrated in tasks such as image fusion [6], low-light image enhancement [7], and high color fidelity preservation [8], further validating their potential for high-fidelity virtual try-on. Particularly, diffusion models have shown significant advantages in handling complex garment deformations and diverse poses, paving new directions for the development of virtual try-on techniques. Inspired by random processes and physical diffusion phenomena, Jascha Sohl-Dickstein et al. first introduced the diffusion process into generative models [28], proposing a generative framework based on diffusion processes. Jonathan Ho et al. developed the Denoising Diffusion Probabilistic Model (DDPM [64]), which progressively restores",
      "chunk_index": "8-0",
      "location": {
        "page_number": 3
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "II. RELATED WORK"
        },
        {
          "level": "Header 3",
          "name": "C. Diffusion Model-Based Virtual Try-On"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "II. RELATED WORK",
        "Header 3": "C. Diffusion Model-Based Virtual Try-On"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "models [28], proposing a generative framework based on diffusion processes. Jonathan Ho et al. developed the Denoising Diffusion Probabilistic Model (DDPM [64]), which progressively restores noise data through a Markov chain, enhancing generative capabilities. Subsequently, Alexander Quinn Nichol et al. [27] optimized the model architecture and training strategy, reducing diffusion steps and significantly accelerating the generation process. Robin Rombach et al. [65] introduced the Latent Diffusion Model (LDM), which transfers the diffusion process to a lower-dimensional latent space, significantly reducing computational costs, accelerating generation speed, and improving computational efficiency and image quality. LDMs have been widely applied in the virtual try-on field. Building on this, Luyang Zhu et al. proposed the TryOnDiffusion architecture [29], which implicitly deforms garment through a cross-attention mechanism and integrates garment deformation with the human image as a",
      "chunk_index": "8-1",
      "location": {
        "page_number": 3
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "II. RELATED WORK"
        },
        {
          "level": "Header 3",
          "name": "C. Diffusion Model-Based Virtual Try-On"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "II. RELATED WORK",
        "Header 3": "C. Diffusion Model-Based Virtual Try-On"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "Luyang Zhu et al. proposed the TryOnDiffusion architecture [29], which implicitly deforms garment through a cross-attention mechanism and integrates garment deformation with the human image as a unified process, effectively preserving garment details while handling pose and body shape variations. Junhong Gou et al. introduced the DCI-VTON model [13], which employs a two-stage process: first, the deformation module preserves local garment details, then the deformed garment is combined with the human image and noise is added as input to the diffusion model, ensuring that the generated image retains more detail. Additionally, Jeongho Kim et al. introduced StableVITON [36], which combines latent diffusion models to learn semantic correspondences in virtual try-on tasks. By incorporating ControlNet [37], this model leverages additional input conditions",
      "chunk_index": "8-2",
      "location": {
        "page_number": 3
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "II. RELATED WORK"
        },
        {
          "level": "Header 3",
          "name": "C. Diffusion Model-Based Virtual Try-On"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "II. RELATED WORK",
        "Header 3": "C. Diffusion Model-Based Virtual Try-On"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "to enhance the accuracy and flexibility of virtual try-on, offering a new solution for efficient and personalized virtual try-on. Mehmet Saygin Seyfioglu et al. proposed the DTC model [26], which enhances image conditioning within latent diffusion models. By integrating detailed reference image features into the latent feature map and utilizing perceptual loss, this model further preserves details, balancing inference speed and high-fidelity detail retention.",
      "chunk_index": "9-0",
      "location": {
        "page_number": 4
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "II. RELATED WORK"
        },
        {
          "level": "Header 3",
          "name": "C. Diffusion Model-Based Virtual Try-On"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "II. RELATED WORK",
        "Header 3": "C. Diffusion Model-Based Virtual Try-On"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "Recent advancements in diffusion models have demonstrated significant potential for text-guided image generation in virtual try-on applications. By iteratively reducing noise to synthesize high-fidelity images conditioned on textual inputs, diffusion models enable personalized try-on image generation aligned with user descriptions. Building upon Latent Diffusion Models (LDM), Davide Morelli et al. proposed LaDI-VTON [23], which integrates LDM with textual inversion to enhance image generation for image-based virtual try-on tasks. To mitigate reconstruction artifacts, LaDI-VTON introduces an augmented autoencoder with learnable skip connections, preserving details outside inpainting regions, and employs a forward textual inversion module for conditional refinement, ensuring texture consistency. Yuha Xu et al. developed the OOTDiffusion model [25], which preprocesses input data to generate human body masks, encodes them into low-dimensional latent variables via a VAE encoder, and",
      "chunk_index": "9-1",
      "location": {
        "page_number": 4
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "II. RELATED WORK"
        },
        {
          "level": "Header 3",
          "name": "C. Diffusion Model-Based Virtual Try-On"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "II. RELATED WORK",
        "Header 3": "C. Diffusion Model-Based Virtual Try-On"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "Yuha Xu et al. developed the OOTDiffusion model [25], which preprocesses input data to generate human body masks, encodes them into low-dimensional latent variables via a VAE encoder, and combines these variables with Gaussian noise as inputs to the denoising network. Concurrently, a U-Net architecture learns garment-specific details to synthesize realistic try-on images. Rui Wang et al. introduced StableGarment [24], which incorporates a garment encoder, adaptive self-attention layers, and a try-on ControlNet to stably preserve fine-grained textile textures while generating stylized images, achieving precise virtual try-on results. These diffusion-based approaches highlight the growing need for fine-grained control and multimodal conditioning in VTON tasks\u2014a direction we pursue with the multimodal prior-guided generation module in HF-VTON.",
      "chunk_index": "9-2",
      "location": {
        "page_number": 4
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "II. RELATED WORK"
        },
        {
          "level": "Header 3",
          "name": "C. Diffusion Model-Based Virtual Try-On"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "II. RELATED WORK",
        "Header 3": "C. Diffusion Model-Based Virtual Try-On"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "Despite significant advancements in image quality and detail preservation, existing virtual try-on (VTON) methods still face several consistency challenges when handling complex garment deformations and diverse poses: **(i) Dependence on Input Quality and Geometric Alignment:** Existing methods heavily rely on high-quality input images and precise geometric alignment, which perform poorly in handling non-rigid garment deformations and pose variations, leading to generated images that lack detail and semantic consistency; **(ii) Multimodal Data Processing and Semantic Consistency:** Although diffusion models show advantages in image generation stability and detail preservation, many approaches still fail to effectively address the challenges of multimodal data processing, garment-body alignment optimization, and maintaining semantic consistency throughout the generation process; **(iii) Detail Preservation and Distortion:** Existing methods often suffer from distortion or information",
      "chunk_index": "10-0",
      "location": {
        "page_number": 4
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "II. RELATED WORK"
        },
        {
          "level": "Header 3",
          "name": "D. Challenges in Existing Virtual Try-On Methods"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "II. RELATED WORK",
        "Header 3": "D. Challenges in Existing Virtual Try-On Methods"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "optimization, and maintaining semantic consistency throughout the generation process; **(iii) Detail Preservation and Distortion:** Existing methods often suffer from distortion or information loss when processing complex garment patterns and details, especially when integrating fine-grained garment features and multimodal data. To address these consistency challenges, we propose the HF-VTON framework, which integrates multiple modules to achieve unified handling of geometric, semantic, and detail consistency.",
      "chunk_index": "10-1",
      "location": {
        "page_number": 4
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "II. RELATED WORK"
        },
        {
          "level": "Header 3",
          "name": "D. Challenges in Existing Virtual Try-On Methods"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "II. RELATED WORK",
        "Header 3": "D. Challenges in Existing Virtual Try-On Methods"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "The HF-VTON framework addresses key challenges in high-fidelity virtual try-on (VTON), focusing on maintaining consistency across geometric alignment, semantic understanding, and fine-grained detail preservation. It comprises three core modules: the Appearance-Preserving Warp Alignment Module (APWAM), the Semantic Representation and Comprehension Module (SRCM), and the Multimodal Prior-Guided Appearance Generation Module (MPAGM). These modules operate in concert to enable precise garment fitting, photorealistic image generation, and robust consistency across diverse poses and garment types, as illustrated in Figure 2.",
      "chunk_index": 11,
      "location": {
        "page_number": 4
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "III. METHODOLOGY"
        },
        {
          "level": "Header 3",
          "name": "A. Overview of the HF-VTON Framework"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "III. METHODOLOGY",
        "Header 3": "A. Overview of the HF-VTON Framework"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "The Appearance-Preserving Warp Alignment Module (APWAM) is designed to achieve precise geometric alignment and preserve appearance details between garment and target human body image in high-fidelity virtual try-on tasks. The module consists of two key components: the Multi-Scale Appearance Representation Extraction (MRE) and the Deformable Flow Estimation Network (DFEN).",
      "chunk_index": "12-0",
      "location": {
        "page_number": 4
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "III. METHODOLOGY"
        },
        {
          "level": "Header 3",
          "name": "B. Appearance-Preserving Warp Alignment Module (APWAM)"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "III. METHODOLOGY",
        "Header 3": "B. Appearance-Preserving Warp Alignment Module (APWAM)"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "**Multi-Scale Appearance Representation Extraction (MRE)** extracts fine-grained appearance details and high-level semantic features from garment and person images, critical for geometric alignment and visual consistency in high-fidelity virtual try-on tasks. To address garment deformation and appearance variations, MRE utilizes a pyramid convolution architecture that progressively captures multi-scale features, from local details to global shape information of both the garment and the person body. The first part of MRE, the **Pyramid Feature Extraction Network (PFEN)**, extracts multi-level features from the input garment image  $I_c$  and person image  $I_p$ , along with 3D human keypoint information  $I_{dense}$  and person segmentation results  $S_p$ . These inputs provide multi-modal data, enabling the extraction of fine-grained texture features  $F_l$ , such as wrinkles, stretching, and material textures. These low-level features serve as a foundation for capturing high-level",
      "chunk_index": "12-1",
      "location": {
        "page_number": 4
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "III. METHODOLOGY"
        },
        {
          "level": "Header 3",
          "name": "B. Appearance-Preserving Warp Alignment Module (APWAM)"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "III. METHODOLOGY",
        "Header 3": "B. Appearance-Preserving Warp Alignment Module (APWAM)"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "data, enabling the extraction of fine-grained texture features  $F_l$ , such as wrinkles, stretching, and material textures. These low-level features serve as a foundation for capturing high-level semantic features  $F_h$  that represent the overall shape of the garment and body structure. The PFEN helps the model understand the garment and person body at various scales, supporting the subsequent appearance flow estimation and geometric alignment. To enhance the fine-grained details of the garment image, MRE integrates  $I_{dense}$  and  $S_p$ , ensuring precise spatial alignment between garment and person images. This spatial information improves geometric accuracy and ensures a better fit between the garment and person body. In the fusion stage, the low-level and high-level features  $F_l$  and  $F_h$  are combined into a unified multi-scale representation  $F_{final}$ , with weights controlled by hyperparameters  $\\alpha$  and  $\\beta$ . This process preserves both fine-grained",
      "chunk_index": "12-2",
      "location": {
        "page_number": 4
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "III. METHODOLOGY"
        },
        {
          "level": "Header 3",
          "name": "B. Appearance-Preserving Warp Alignment Module (APWAM)"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "III. METHODOLOGY",
        "Header 3": "B. Appearance-Preserving Warp Alignment Module (APWAM)"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "$F_l$  and  $F_h$  are combined into a unified multi-scale representation  $F_{final}$ , with weights controlled by hyperparameters  $\\alpha$  and  $\\beta$ . This process preserves both fine-grained details and semantic information, providing accurate features for the subsequent appearance flow estimation and geometric alignment.",
      "chunk_index": "12-3",
      "location": {
        "page_number": 4
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "III. METHODOLOGY"
        },
        {
          "level": "Header 3",
          "name": "B. Appearance-Preserving Warp Alignment Module (APWAM)"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "III. METHODOLOGY",
        "Header 3": "B. Appearance-Preserving Warp Alignment Module (APWAM)"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "**Deformable Flow Estimation Network (DFEN)** is designed to refine the appearance flow between garment and the person body, addressing complex deformations. It consists of  $N$  cascaded flow networks, each progressively enhancing the flow accuracy. At each layer, DFEN takes as input the previous layer's flow  $p_{i-1}$ , the current pyramid features  $c_i$ , and calculates the correlation loss  $\\text{corr}(p_i, c_i)$  to ensure semantic consistency. These features are then passed through a deformable convolution module to predict the flow residual  $\\Delta \\text{flow}_i$ :\n\n$$\\Delta \\text{flow}_i = \\text{DeformConv}(p_i, c_i, \\text{corr}(p_i, c_i))$$",
      "chunk_index": "12-4",
      "location": {
        "page_number": 4
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "III. METHODOLOGY"
        },
        {
          "level": "Header 3",
          "name": "B. Appearance-Preserving Warp Alignment Module (APWAM)"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "III. METHODOLOGY",
        "Header 3": "B. Appearance-Preserving Warp Alignment Module (APWAM)"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "![Figure 2: Overview of the proposed HF-VTON framework. The diagram shows three main modules: Semantic Representation and Comprehension Module (SRM), Appearance-Preserving Warp Alignment Module (APWAM), and Multimodal Prior-Guided Appearance Generation Module (MPAGM). The SRM uses Starfire and CogVlm2 to process garment attributes. The APWAM uses SAMP-VTONS dataset and a Deformable Flow Estimation Network (DFEN) to align garment and body images. The MPAGM combines visual and semantic priors via Cross-Modal Semantic and Visual Fusion and a Denoising UNet to generate the final virtual try-on image I_tryon.](690fce4fb5c9cbb8beb560cb2a3fcbeb_img.jpg)",
      "chunk_index": "13-0",
      "location": {
        "page_number": 5
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "III. METHODOLOGY"
        },
        {
          "level": "Header 3",
          "name": "B. Appearance-Preserving Warp Alignment Module (APWAM)"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "III. METHODOLOGY",
        "Header 3": "B. Appearance-Preserving Warp Alignment Module (APWAM)"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "Figure 2: Overview of the proposed HF-VTON framework. The diagram shows three main modules: Semantic Representation and Comprehension Module (SRM), Appearance-Preserving Warp Alignment Module (APWAM), and Multimodal Prior-Guided Appearance Generation Module (MPAGM). The SRM uses Starfire and CogVlm2 to process garment attributes. The APWAM uses SAMP-VTONS dataset and a Deformable Flow Estimation Network (DFEN) to align garment and body images. The MPAGM combines visual and semantic priors via Cross-Modal Semantic and Visual Fusion and a Denoising UNet to generate the final virtual try-on image I\\_tryon.",
      "chunk_index": "13-1",
      "location": {
        "page_number": 5
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "III. METHODOLOGY"
        },
        {
          "level": "Header 3",
          "name": "B. Appearance-Preserving Warp Alignment Module (APWAM)"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "III. METHODOLOGY",
        "Header 3": "B. Appearance-Preserving Warp Alignment Module (APWAM)"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "Fig. 2. Overview of the proposed HF-VTON framework for high-fidelity virtual try-on. HF-VTON integrates three core modules to achieve consistency across geometric alignment, semantic understanding, and fine-grained detail preservation. The Appearance-Preserving Warp Alignment Module (APWAM) ensures accurate garment-body alignment in various poses by capturing both local and global geometric features. The Semantic Representation and Comprehension Module (SRCM) improves semantic consistency by enhancing garment attribute understanding and aligning them with body poses. The Multimodal Prior-Guided Appearance Generation Module (MPAGM) combines geometric priors with semantic and visual cues to generate high-fidelity virtual try-on images, maintaining garment structure and appearance consistency across different garment types and poses.",
      "chunk_index": "13-2",
      "location": {
        "page_number": 5
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "III. METHODOLOGY"
        },
        {
          "level": "Header 3",
          "name": "B. Appearance-Preserving Warp Alignment Module (APWAM)"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "III. METHODOLOGY",
        "Header 3": "B. Appearance-Preserving Warp Alignment Module (APWAM)"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "Deformable convolution introduces dynamic sampling by learning offsets  $\\Delta p_n$ , enabling the network to adapt to local deformations such as wrinkles and stretching. This flexibility allows the network to better handle non-rigid garment deformations compared to traditional convolution with fixed kernels. The mathematical formulation of deformable convolution is:\n\n$$y(p_i) = \\sum_{n \\in R} w(p_n) \\cdot x(p_i + p_n + \\Delta p_n)$$\n\nwhere  $p_i$  is the flow field of the current layer,  $p_n$  is the sampling position of the convolution kernel,  $\\Delta p_n$  is the learned offset, enabling the convolution kernels to adjust based on the input\u2019s geometric changes. The refined flow  $\\Delta \\text{flow}_i$  is then added to the previous flow  $p_{i-1}$ , resulting in the updated flow  $p_i$ :\n\n$$p_i = p_{i-1} \\oplus \\Delta \\text{flow}_i$$",
      "chunk_index": "13-3",
      "location": {
        "page_number": 5
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "III. METHODOLOGY"
        },
        {
          "level": "Header 3",
          "name": "B. Appearance-Preserving Warp Alignment Module (APWAM)"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "III. METHODOLOGY",
        "Header 3": "B. Appearance-Preserving Warp Alignment Module (APWAM)"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "$$p_i = p_{i-1} \\oplus \\Delta \\text{flow}_i$$\n\nBy progressively refining the flow and incorporating deformable convolution, DFEN accurately captures garment deformations across different poses, improving the visual quality and alignment for high-fidelity virtual try-on.",
      "chunk_index": "13-4",
      "location": {
        "page_number": 5
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "III. METHODOLOGY"
        },
        {
          "level": "Header 3",
          "name": "B. Appearance-Preserving Warp Alignment Module (APWAM)"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "III. METHODOLOGY",
        "Header 3": "B. Appearance-Preserving Warp Alignment Module (APWAM)"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "The Semantic Representation and Comprehension Module (SRCM) aims to improve the semantic understanding and representation of garments in virtual try-on. Existing approaches generally focus on low-level visual feature alignment but often overlook the semantic structuring of garment attributes and their alignment with human poses. To address these limitations, SRCM integrates three key components: Semantic Attribute Structuring (SAS), Dual-Model Collaborative Text Generation (DMTG), and Semantic-Aligned Multi-Pose Virtual Try-On Dataset Construction (SAMP-VTONS). This integration enhances semantic understanding in virtual try-on and ensures consistent alignment between garments and\n\nhuman poses across various poses.",
      "chunk_index": "14-0",
      "location": {
        "page_number": 5
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "III. METHODOLOGY"
        },
        {
          "level": "Header 3",
          "name": "C. Semantic Representation and Comprehension Module (SRCM)"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "III. METHODOLOGY",
        "Header 3": "C. Semantic Representation and Comprehension Module (SRCM)"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "**Semantic Attribute Structuring for Garments (SAS)** module provides a systematic and structured representation of upper-body garment attributes. Building on the hierarchical attribute tree from the FashionAI dataset [42], SAS refines and extends the attribute representation for garments. While FashionAI offers basic attribute classification for upper and lower body garments, it lacks detailed coverage of fine-grained garment features such as fit, pattern, and color. To address this, SAS introduces three additional attribute categories\u2014fit, pattern, and color and refines garment attributes into seven main categories: **Fit** (slim, loose, straight), **Pattern**, **Color**, **Neckline Design** (low, mid, high), **Collar Design** (V-neck, deep V-neck, round neck, square neck, irregular), **Sleeve Length** (sleeveless, short sleeve, long sleeve), and **Shirt Length** (high waist, normal, long, extra-long). This extension provides a more comprehensive and precise representation of the",
      "chunk_index": "14-1",
      "location": {
        "page_number": 5
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "III. METHODOLOGY"
        },
        {
          "level": "Header 3",
          "name": "C. Semantic Representation and Comprehension Module (SRCM)"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "III. METHODOLOGY",
        "Header 3": "C. Semantic Representation and Comprehension Module (SRCM)"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "**Sleeve Length** (sleeveless, short sleeve, long sleeve), and **Shirt Length** (high waist, normal, long, extra-long). This extension provides a more comprehensive and precise representation of the appearance and design elements of the garment, allowing accurate textual descriptions for virtual tests and improving semantic understanding and precision.",
      "chunk_index": "14-2",
      "location": {
        "page_number": 5
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "III. METHODOLOGY"
        },
        {
          "level": "Header 3",
          "name": "C. Semantic Representation and Comprehension Module (SRCM)"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "III. METHODOLOGY",
        "Header 3": "C. Semantic Representation and Comprehension Module (SRCM)"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "**Dual-Model Collaborative Text Generation for Garments (DMTG)** addresses key challenges in the generation of garment text for virtual tests, specifically in terms of precision, speed, and compliance. While commercial models (e.g., ChatGPT-4.0 and Wenxin Yiyan) offer high accuracy, their API costs limit practical deployment. Open-source models (e.g., CogVlm2) provide better customization but suffer from slower generation speeds. In contrast, production-grade APIs (e.g., Starfire) excel in speed but are constrained by content safety filters, which prevent the generation of text descriptions for images containing sensitive content such as flags, logos, or exaggerated garment patterns. To mitigate these issues, we propose a dual-model collaborative strategy, using the Starfire model as the primary generator for fast response times, while leveraging CogVlm2 as a secondary model to handle edge cases and provide more detailed text descriptions. Additionally, we implement a regular",
      "chunk_index": "14-3",
      "location": {
        "page_number": 5
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "III. METHODOLOGY"
        },
        {
          "level": "Header 3",
          "name": "C. Semantic Representation and Comprehension Module (SRCM)"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "III. METHODOLOGY",
        "Header 3": "C. Semantic Representation and Comprehension Module (SRCM)"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "as the primary generator for fast response times, while leveraging CogVlm2 as a secondary model to handle edge cases and provide more detailed text descriptions. Additionally, we implement a regular expression parser and",
      "chunk_index": "14-4",
      "location": {
        "page_number": 5
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "III. METHODOLOGY"
        },
        {
          "level": "Header 3",
          "name": "C. Semantic Representation and Comprehension Module (SRCM)"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "III. METHODOLOGY",
        "Header 3": "C. Semantic Representation and Comprehension Module (SRCM)"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "pruning mechanism to remove API metadata and non-semantic tokens, further enhancing the precision and efficiency of the generated descriptions.",
      "chunk_index": "15-0",
      "location": {
        "page_number": 6
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "III. METHODOLOGY"
        },
        {
          "level": "Header 3",
          "name": "C. Semantic Representation and Comprehension Module (SRCM)"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "III. METHODOLOGY",
        "Header 3": "C. Semantic Representation and Comprehension Module (SRCM)"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "**Semantic-Aligned Multi-Pose Virtual Try-On Dataset Construction (SAMP-VTONS)** is a critical component of our proposed virtual try-on framework, addressing the limitations of existing datasets in terms of pose diversity and cross-modal semantic alignment. Existing datasets, such as VITON-HD [1] and Dress Code [9], predominantly focus on single-pose images and fail to adequately capture the adaptability and deformation of garment across different poses, limiting the performance and accuracy of virtual try-on models in multi-pose scenarios. While the FashionTryOn [11] dataset provides two poses per subject, making it suitable for multi-pose virtual try-on tasks, it still lacks sufficient pose diversity and cross-modal semantic alignment. To address these issues, we construct the SAMP-VTONS (Semantic-Aligned Multi-Pose Virtual Try-On) dataset, which incorporates multi-pose images with textual annotations. Based on FashionTryOn, the dataset employs DensePose [43] and OpenPose [44] for",
      "chunk_index": "15-1",
      "location": {
        "page_number": 6
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "III. METHODOLOGY"
        },
        {
          "level": "Header 3",
          "name": "C. Semantic Representation and Comprehension Module (SRCM)"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "III. METHODOLOGY",
        "Header 3": "C. Semantic Representation and Comprehension Module (SRCM)"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "(Semantic-Aligned Multi-Pose Virtual Try-On) dataset, which incorporates multi-pose images with textual annotations. Based on FashionTryOn, the dataset employs DensePose [43] and OpenPose [44] for accurate pose estimation, ensuring precise alignment of each garment image with human images in various poses. Next, we use the Human Parse [45] method for fine-grained segmentation of the human body, labeling the distinct body parts, while the Parse Agnostic method [1] removes identity information and non-try-on areas, minimizing background interference in the virtual try-on process. Finally, our DMTG generates accurate textual descriptions for each garment image, forming a semantically aligned triplet of image, pose, and text. The SAMP-VTONS dataset consists of 21,104 training images and 7,487 test images. Each sample includes: human pose images, garment images, garment mask images, garment attribute textual descriptions, pose estimates (DensePose and OpenPose), human body part",
      "chunk_index": "15-2",
      "location": {
        "page_number": 6
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "III. METHODOLOGY"
        },
        {
          "level": "Header 3",
          "name": "C. Semantic Representation and Comprehension Module (SRCM)"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "III. METHODOLOGY",
        "Header 3": "C. Semantic Representation and Comprehension Module (SRCM)"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "and 7,487 test images. Each sample includes: human pose images, garment images, garment mask images, garment attribute textual descriptions, pose estimates (DensePose and OpenPose), human body part segmentation (Human Parse), and identity-agnostic processing (Human Agnostic). By enhancing pose diversity and achieving semantic alignment across images, poses, and texts, it provides superior data support for the training and evaluation of multi-pose virtual try-on models.",
      "chunk_index": "15-3",
      "location": {
        "page_number": 6
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "III. METHODOLOGY"
        },
        {
          "level": "Header 3",
          "name": "C. Semantic Representation and Comprehension Module (SRCM)"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "III. METHODOLOGY",
        "Header 3": "C. Semantic Representation and Comprehension Module (SRCM)"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "The Multimodal Prior-Guided Appearance Generation Module (MPAGM) is a pivotal component designed to enhance the quality and accuracy of virtual try-on by effectively integrating geometric and multimodal geometric priors with advanced generative techniques. Existing methods typically struggle with insufficient integration of textual, visual, and geometric features, limiting their ability to generate realistic garment fittings with fine-grained details. To address these shortcomings, MPAGM introduces three tightly integrated submodules: Geometric-Conditioned Multimodal Fusion (GCMF), Cross-Modal Semantic and Visual Fusion (CSVF), and Dual-Conditioned Guidance Appearance Generation (DGAG).",
      "chunk_index": "16-0",
      "location": {
        "page_number": 6
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "III. METHODOLOGY"
        },
        {
          "level": "Header 3",
          "name": "D. Multimodal Prior-Guided Appearance Generation Module (MPAGM)"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "III. METHODOLOGY",
        "Header 3": "D. Multimodal Prior-Guided Appearance Generation Module (MPAGM)"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "**Geometric-Conditioned Multimodal Fusion** integrates garment deformation features, identity-agnostic human representations, and structured pose information to provide spatially consistent geometric priors for the diffusion model to enhance the accuracy and consistency of virtual try-on image generation. First, we perform feature encoding on the target garment deformation features  $I_{warp}$  using the pre-trained Stable Diffusion Encoder  $E_{SD}$ , extracting multi-scale features and producing the encoded deformation feature  $E_{warp}$ :\n\n$$E_{warp} = E_{SD}(I_{warp}) \\in \\mathbb{R}^{c_e \\times h \\times w}, \\quad h = \\frac{H}{8}, \\quad w = \\frac{W}{8}, \\quad c_e = 4$$",
      "chunk_index": "16-1",
      "location": {
        "page_number": 6
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "III. METHODOLOGY"
        },
        {
          "level": "Header 3",
          "name": "D. Multimodal Prior-Guided Appearance Generation Module (MPAGM)"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "III. METHODOLOGY",
        "Header 3": "D. Multimodal Prior-Guided Appearance Generation Module (MPAGM)"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "$$E_{warp} = E_{SD}(I_{warp}) \\in \\mathbb{R}^{c_e \\times h \\times w}, \\quad h = \\frac{H}{8}, \\quad w = \\frac{W}{8}, \\quad c_e = 4$$\n\nwhere  $c_e$  is the number of channels in the latent space and  $h, w$  are the downsampled spatial dimensions. Next, we perform similar encoding on the identity-agnostic human features  $I_{agnostic}$  using  $E_{SD}$ , yielding the encoded identity-agnostic feature  $E_{agnostic}$ . To incorporate spatial constraints, we utilize the binary mask  $m$  and pose map  $p$ , which provide explicit guidance for the region of interest in the human body. These constraints allow the generation model to focus on the relevant parts of the image, reducing unwanted artifacts. Additionally, we introduce the noise latent variable  $z \\sim \\mathcal{N}(0, I)$ . We concatenate the aforementioned five input features into a single representation space  $\\Gamma_{gp} = [E_{warp}, E_{agnostic}, m, p, z]$  providing rich geometric priors for the diffusion model.",
      "chunk_index": "16-2",
      "location": {
        "page_number": 6
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "III. METHODOLOGY"
        },
        {
          "level": "Header 3",
          "name": "D. Multimodal Prior-Guided Appearance Generation Module (MPAGM)"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "III. METHODOLOGY",
        "Header 3": "D. Multimodal Prior-Guided Appearance Generation Module (MPAGM)"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "**Cross-Modal Semantic and Visual Fusion** integrates semantic and visual representations by constructing a joint embedding space, aligning and merging high-quality textual descriptions with garment visual features to ensure semantic consistency and fine-grained detail preservation. Initially, the target garment image  $C \\in \\mathbb{R}^{3 \\times H \\times W}$  is passed through the CLIP visual encoder  $V_E$  to extract visual features  $E_{vis}$ , capturing key garment attributes such as texture, shape, and color. Then  $E_{vis}$  are processed by the network  $F_\\theta$ , consisting of a Vision Transformer (ViT) and a Multilayer Perceptron (MLP), mapping the extracted features to the CLIP text space and generating the predicted pseudo-word embedding  $V_{pseudo}$ . Concurrently, the garment's high-quality textual description, generated by the SRCM module, is processed through Tokenizer and Embedding Lookup to produce the text embedding vector  $T_{feature} = \\{T_{version},",
      "chunk_index": "16-3",
      "location": {
        "page_number": 6
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "III. METHODOLOGY"
        },
        {
          "level": "Header 3",
          "name": "D. Multimodal Prior-Guided Appearance Generation Module (MPAGM)"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "III. METHODOLOGY",
        "Header 3": "D. Multimodal Prior-Guided Appearance Generation Module (MPAGM)"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "the garment's high-quality textual description, generated by the SRCM module, is processed through Tokenizer and Embedding Lookup to produce the text embedding vector  $T_{feature} = \\{T_{version}, T_{pattern}, T_{color}, T_{collar}, T_{neckline}, T_{sleeve}, T_{length}\\}$ . By aligning and transforming the respective feature spaces, textual and visual representations are effectively fused, yielding a joint embedding  $Y \\in \\mathbb{R}^{d_{joint}} = \\text{Concat}(V_{pseudo}, T_{feature})$ . Finally,  $Y$  is passed to the CLIP text encoder  $T_E$ , generating the final text embedding  $E_{text} \\in \\mathbb{R}^{d_{text}}$ . This embedding serves as the conditioning signal for the diffusion model, guiding the generation of the virtual try-on image.",
      "chunk_index": "16-4",
      "location": {
        "page_number": 6
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "III. METHODOLOGY"
        },
        {
          "level": "Header 3",
          "name": "D. Multimodal Prior-Guided Appearance Generation Module (MPAGM)"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "III. METHODOLOGY",
        "Header 3": "D. Multimodal Prior-Guided Appearance Generation Module (MPAGM)"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "**Dual-Conditioned Guidance Appearance Generation** uses the denoising UNet as the core network, taking as input the geometric condition information  $\\Gamma_{gp}$  from the GCMF and the semantic visual information  $E_{text}$  from the CSVF. During each denoising step,  $E_{text}$  serves as the guiding signal, ensuring that the generated image aligns with the target textual description. As the de-noising process progresses, the network gradually synthesizes the virtual try-on image  $I_{trv}$  by iteratively integrating both image and text features. To optimize the generation process, the model trains the denoising network  $\\epsilon_\\theta$  by minimizing the following loss function:\n\n$$L_{DM} = \\mathbb{E}_{I_g, p, \\epsilon \\sim \\mathcal{N}(0, 1), t} [\\|\\epsilon - \\epsilon_\\theta(I_g, t, \\tau_\\theta(Y))\\|^2] \\quad (1)$$",
      "chunk_index": "16-5",
      "location": {
        "page_number": 6
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "III. METHODOLOGY"
        },
        {
          "level": "Header 3",
          "name": "D. Multimodal Prior-Guided Appearance Generation Module (MPAGM)"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "III. METHODOLOGY",
        "Header 3": "D. Multimodal Prior-Guided Appearance Generation Module (MPAGM)"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "$$L_{DM} = \\mathbb{E}_{I_g, p, \\epsilon \\sim \\mathcal{N}(0, 1), t} [\\|\\epsilon - \\epsilon_\\theta(I_g, t, \\tau_\\theta(Y))\\|^2] \\quad (1)$$\n\nwhere  $t$  represents the diffusion timestep,  $\\tau_\\theta(Y)$  is the conditional encoding of the text description  $Y$ , and  $\\epsilon_\\theta$  is the denoising network. The core objective of this loss function is to minimize the difference between the noisy image  $I_g$  and the predicted network output  $\\epsilon_\\theta(I_g, t, \\tau_\\theta(Y))$ , while accounting for the integration of image and text feature spaces. By optimizing this loss, the model is able to generate high-fidelity images that closely align with the input textual descriptions. It effectively guides the model to generate images consistent with both geometric features and semantic information. This dual-conditioned guidance improves the precision of the generated virtual try-on images, ensuring that the output matches the expected garment style, fit, and appearance.",
      "chunk_index": "16-6",
      "location": {
        "page_number": 6
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "III. METHODOLOGY"
        },
        {
          "level": "Header 3",
          "name": "D. Multimodal Prior-Guided Appearance Generation Module (MPAGM)"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "III. METHODOLOGY",
        "Header 3": "D. Multimodal Prior-Guided Appearance Generation Module (MPAGM)"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "**Datasets.** We conduct extensive experiments on the widely used VITON-HD benchmark and our newly proposed SAMP-VTON dataset to comprehensively evaluate the robustness of HF-VTON across various challenging scenarios. (i) The VITON-HD dataset, collected by Choi et al. [1], is specifically designed for virtual try-on tasks and includes 13,679 front-facing images of women paired with corresponding upper-body garment images. It provides pose annotations such as OpenPose and DensePose, as well as garment masks for accurate fitting. Although the original dataset does not include textual descriptions, we generate them using our DMTG module from SRCM. The dataset is split into 11,647 training samples and 2,032 test samples, with a resolution of 786x1024, resized to 384x512 for consistency in our experiments. (ii) The SAMP-VTONS dataset is an extension of the FashionTryOn dataset, consisting of 21,104 training samples and 7,487 test samples. Each sample includes two images in different poses,",
      "chunk_index": "17-0",
      "location": {
        "page_number": 7
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "A. Experimental Setup"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "A. Experimental Setup"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "experiments. (ii) The SAMP-VTONS dataset is an extension of the FashionTryOn dataset, consisting of 21,104 training samples and 7,487 test samples. Each sample includes two images in different poses, accompanied by 200,137 paired textual descriptions. The images are of resolution 384x512, designed to evaluate multi-pose garment fitting and semantic alignment tasks. Both datasets are essential for evaluating the performance of HF-VTON and comparing it with existing methods, with VITON-HD focusing on pose-annotated garment fitting and SAMP-VTONS offering a multi-pose garment fitting scenario.",
      "chunk_index": "17-1",
      "location": {
        "page_number": 7
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "A. Experimental Setup"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "A. Experimental Setup"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "**Evaluation Metrics.** To thoroughly assess the performance of our model, we employed multiple evaluation metrics. Specifically, we use the Structural Similarity Index (SSIM) [2] to quantify image similarity in terms of brightness, contrast, and structural consistency. To capture perceptual similarity, we use Learned Perceptual Image Patch Similarity (LPIPS) [60], which reflects the human visual system\u2019s sensitivity to image differences. Additionally, we incorporate Fr\u00e9chet Inception Distance (FID) [61] and Kernel Inception Distance (KID) [62] to assess the quality of the generative model. These metrics compare the feature distributions of generated images against real images, providing a comprehensive evaluation of the model\u2019s capability in generating high-quality outputs.",
      "chunk_index": "17-2",
      "location": {
        "page_number": 7
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "A. Experimental Setup"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "A. Experimental Setup"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "**Implementation Details.** The experiments are conducted on a system with an A800 GPU (80GB of memory) and a Linux-based environment, using Python 3.8, PyTorch 2.0.1, and CUDA 11.8 for both training and testing. The core modules, APWAM, CSVF, and DGAG, are implemented as follows: For APWAM, which generates deformed garments as input for synthesis, we use the Adam optimizer [47] to train the deformation network for 100 epochs, with  $\\beta_1 = 0.5$  and  $\\beta_2 = 0.999$ . The initial learning rate is set to 0.00005, decaying linearly to zero from epoch 50. The loss function hyperparameters are set to  $\\lambda_1 = 0.2$ ,  $\\lambda_2 = 0.01$ , and  $\\lambda_3 = 6$ . For CSVF, within the MPAGM, we generate 16 pseudo-word embeddings and train the model for 150,000 steps with a learning rate of 0.00001. The optimizer is AdamW, with  $\\beta_1 = 0.9$ ,  $\\beta_2 = 0.999$ , and weight decay set to 0.01. The OpenCLIP ViT-H/14 model [48], pre-trained on the LAION-2B dataset [49], serves as",
      "chunk_index": "17-3",
      "location": {
        "page_number": 7
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "A. Experimental Setup"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "A. Experimental Setup"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "of 0.00001. The optimizer is AdamW, with  $\\beta_1 = 0.9$ ,  $\\beta_2 = 0.999$ , and weight decay set to 0.01. The OpenCLIP ViT-H/14 model [48], pre-trained on the LAION-2B dataset [49], serves as the visual encoder. In the DGAG component, we train for 150,000 iterations, randomly masking text, deformed garment, and pose inputs with a probability of 0.2 to enhance robustness. During inference, a non-classifier guided technique [51] is applied to ensure high authenticity and consistency in the generated images under multiple constraints. Lastly, following [52], we utilize a fast variant of multicondition nonclassifier guidance, optimizing",
      "chunk_index": "17-4",
      "location": {
        "page_number": 7
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "A. Experimental Setup"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "A. Experimental Setup"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "TABLE I\n\nCOMPARISON WITH STATE-OF-THE-ART VIRTUAL TRY-ON METHODS ON THE SAMP-VTONS AND VITON-HD DATASETS. THE METRICS USE  $\\uparrow/\\downarrow$  TO INDICATE THAT LARGER/SMALLER VALUES CORRESPOND TO BETTER PERFORMANCE. THE TOP TWO RESULTS ARE HIGHLIGHTED IN A SPECIFIC ORDER: **RED** FOR THE BEST RESULT AND **GREEN** FOR THE SECOND-BEST RESULT.",
      "chunk_index": "17-5",
      "location": {
        "page_number": 7
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "A. Experimental Setup"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "A. Experimental Setup"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "| Method                    | Paired          |                    |                  |                  | Unpaired         |                  |\n|---------------------------|-----------------|--------------------|------------------|------------------|------------------|------------------|\n|                           | SSIM $\\uparrow$ | LPIPS $\\downarrow$ | KID $\\downarrow$ | FID $\\downarrow$ | KID $\\downarrow$ | FID $\\downarrow$ |\n| <b>VITON-HD Dataset</b>   |                 |                    |                  |                  |                  |                  |\n| PF-AFN(CVPR) [14]         | 0.845           | 0.159              | 1.641            | 26.11            | 2.083            | 30.37            |\n| VITON-HD(CVPR) [1]        | <b>0.856</b>    | 0.081              | 0.404            | 11.69            | 0.445            | 13.52            |\n| LaDI-VTON(ACM MM) [23]    | 0.846           | 0.103              | 1.123            | <b>6.29</b>      | <b>0.265</b>     | <b>11.08</b>     |\n| DM-VTON(ISMAR) [50]       | 0.850           | 0.151              | 1.335            | 21.85            | 1.596            | 24.82            |\n| MV-VTON(AAAI) [34]        | <b>0.873</b>    | <b>0.060</b>       | <b>0.135</b>     | 6.76             | 0.427            | 12.32            |\n| HF-VTON (Ours)            | 0.852           | <b>0.050</b>       | <b>0.024</b>     | <b>5.51</b>      | <b>0.196</b>     | <b>10.12</b>     |\n| <b>SAMP-VTONS Dataset</b> |                 |                    |                  |                  |                  |                  |\n| PF-AFN(CVPR) [14]         | 0.843           | 0.175              | 3.766            | 43.92            | 4.590            | 49.31            |\n| VITON-HD(CVPR) [1]        | 0.852           | 0.101              | 0.382            | 9.26             | 0.771            | 13.10            |\n| DCI-VTON(ACM MM) [13]     | <b>0.886</b>    | <b>0.073</b>       | <b>0.189</b>     | <b>5.47</b>      | 0.307            | 8.15             |\n| DM-VTON(ISMAR) [50]       | 0.844           | 0.181              | 3.158            | 35.97            | 3.704            | 39.72            |\n| MV-VTON(AAAI) [34]        | 0.869           | 0.096              | 0.220            | 6.10             | <b>0.263</b>     | <b>7.38</b>      |\n| HF-VTON (Ours)            | <b>0.892</b>    | <b>0.043</b>       | <b>0.084</b>     | <b>3.35</b>      | <b>0.256</b>     | <b>6.92</b>      |",
      "chunk_index": 18,
      "location": {
        "page_number": 7
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "A. Experimental Setup"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "A. Experimental Setup",
        "type": "table"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "computational efficiency by ensuring that the final results\u2019 complexity is independent of the number of input constraints.",
      "chunk_index": 19,
      "location": {
        "page_number": 7
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "A. Experimental Setup"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "A. Experimental Setup"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "We conduct a comprehensive evaluation of HF-VTON against state-of-the-art virtual try-on methods on the VITON-HD and SAMP-VTONS datasets. In the VITON-HD dataset, we first compare HF-VTON with the baseline method VITON-HD [1], followed by comparisons with PF-AFN, LaDI-VTON, DM-VTON [50] and MV-VTON [34], ensuring consistent training protocols and evaluation metrics across all methods. Next, in the SAMP-VTONS dataset, we compare HF-VTON with five advanced methods: PF-AFN [14] (geometric alignment), DCI-VTON [13] (diffusion-based synthesis), LaDI-VTON (latent diffusion modeling), DM-VTON (multimodal fusion) and MV-VTON (multiview rendering), which represent different technical paradigms in virtual try-on research.",
      "chunk_index": "20-0",
      "location": {
        "page_number": 7
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "B. Comparisons with State-of-the-art Methods"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "B. Comparisons with State-of-the-art Methods"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "**Quantitative Comparison.** Table I presents the comprehensive quantitative evaluation of HF-VTON on the VITON-HD and SAMP-VTONS datasets, covering paired and unpaired settings. In the paired setting on the VITON-HD dataset, HF-VTON achieves an FID of 5.51 and a KID of 0.024, representing a 12.4% improvement over LaDI-VTON (6.29) and an 18.5% improvement over MV-VTON (6.76). KID is reduced by 82.1% compared to MV-VTON (0.135  $\\to$  0.024), demonstrating the superiority of HF-VTON in image quality and consistency of the generative distribution. With an LPIPS of 0.050, HF-VTON outperforms LaDI-VTON (0.103) and MV-VTON (0.060), indicating enhanced preservation of texture detail. In the unpaired setting, HF-VTON maintains robust performance with an FID of 10.12 and a KID of 0.196, surpassing LaDI-VTON (FID: 11.08, KID: 0.265) and MV-VTON (FID: 12.32, KID: 0.427), highlighting its strong generalizability to pose variations and data distribution shifts.",
      "chunk_index": "20-1",
      "location": {
        "page_number": 7
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "B. Comparisons with State-of-the-art Methods"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "B. Comparisons with State-of-the-art Methods"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "On the single-pose scenario of the SAMP-VTONS dataset, HF-VTON achieves the highest SSIM of 0.892 and the lowest LPIPS of 0.043, outperforming DCI-VTON (SSIM: 0.886, LPIPS: 0.073) and MV-VTON (SSIM: 0.869, LPIPS: 0.096), excelling in structure preservation and texture fidelity. In the unpaired setting, HF-VTON further reduced the FID to 6.92 and KID to 0.256, surpassing MV-VTON (FID: 7.38, KID: 0.263) and DCI-VTON (FID: 8.15, KID: 0.307), further validating its superior image quality, structural restoration, and detail retention in standard single-pose tasks.\n\n**Qualitative Comparison.** Figure 3 presents a qualitative",
      "chunk_index": "20-2",
      "location": {
        "page_number": 7
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "B. Comparisons with State-of-the-art Methods"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "B. Comparisons with State-of-the-art Methods"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "![Figure 3: Qualitative comparison results on the VITON-HD dataset. The figure is a grid of 16 images arranged in 4 rows and 8 columns. The first column shows the 'Garment' image, the second column shows the 'Person' image, and the remaining six columns show results from PF-AFN, VTON-HD, LaDI-VTON, DM-VTON, and MV-VTON. The final column, labeled 'HF-VTON (Ours)', shows the results of the proposed method. Red boxes highlight areas with poor performance (misalignment, distortion) in the competing methods, while green boxes highlight areas where HF-VTON excels (accurate alignment, detail preservation).](892f25e3d71d8e315a2a51092a8a8da7_img.jpg)",
      "chunk_index": "21-0",
      "location": {
        "page_number": 8
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "B. Comparisons with State-of-the-art Methods"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "B. Comparisons with State-of-the-art Methods"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "Figure 3: Qualitative comparison results on the VITON-HD dataset. The figure is a grid of 16 images arranged in 4 rows and 8 columns. The first column shows the 'Garment' image, the second column shows the 'Person' image, and the remaining six columns show results from PF-AFN, VTON-HD, LaDI-VTON, DM-VTON, and MV-VTON. The final column, labeled 'HF-VTON (Ours)', shows the results of the proposed method. Red boxes highlight areas with poor performance (misalignment, distortion) in the competing methods, while green boxes highlight areas where HF-VTON excels (accurate alignment, detail preservation).\n\nFig. 3. Qualitative comparison results on the VITON-HD dataset are presented in the following columns: Garment image, Person image, and results from five state-of-the-art virtual try-on methods. Red boxes highlight errors, while green boxes emphasize the result of ours.",
      "chunk_index": "21-1",
      "location": {
        "page_number": 8
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "B. Comparisons with State-of-the-art Methods"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "B. Comparisons with State-of-the-art Methods"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "comparison of HF-VTON with state-of-the-art virtual try-on methods on the VITON-HD dataset. The figure includes garment images (first column), person images (second column), and the results of six competing methods (columns 3 to 8). Red boxes highlight areas with poor performance, while green boxes emphasize the advantages of HF-VTON in detail restoration and alignment. Existing methods exhibit significant alignment errors, particularly in the shoulder and sleeve regions, resulting in unnatural garment-body fitting. In contrast, HF-VTON accurately aligns the garment with the body, ensuring a smooth transition of garment contours and poses. When handling complex textures such as stripes and patterns, existing methods often suffer from blurriness or distortion, especially in detail preservation. HF-VTON, however, maintains clear texture retention, particularly excelling in preserving pattern integrity and the natural folds of fabric. Overall, HF-VTON significantly improves image quality",
      "chunk_index": "21-2",
      "location": {
        "page_number": 8
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "B. Comparisons with State-of-the-art Methods"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "B. Comparisons with State-of-the-art Methods"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "HF-VTON, however, maintains clear texture retention, particularly excelling in preserving pattern integrity and the natural folds of fabric. Overall, HF-VTON significantly improves image quality and structural consistency through its representation consistency framework, showcasing notable advantages in complex garment deformation, fine-grained detail fidelity, and precise garment-body alignment. In the single-pose scenario of the SAMP-VTONS dataset, HF-VTON also demonstrates significant advantages over existing methods, as shown in Figure 4. LaDI-VTON exhibits noticeable texture distortion, leading to a mismatch between the garment and the original image. Other methods suffer from considerable alignment errors in critical areas such as the shoulders and upper arms, resulting in unnatural garment-body fitting. HF-VTON achieves precise alignment between the garment and the body, particularly in challenging regions, ensuring a natural transition. While MV-VTON performs relatively well,",
      "chunk_index": "21-3",
      "location": {
        "page_number": 8
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "B. Comparisons with State-of-the-art Methods"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "B. Comparisons with State-of-the-art Methods"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "garment-body fitting. HF-VTON achieves precise alignment between the garment and the body, particularly in challenging regions, ensuring a natural transition. While MV-VTON performs relatively well, it still exhibits discrepancies in texture detail preservation, especially in garment detail restoration. HF-VTON excels in maintaining fine details, particularly in handling complex garment deformations and texture fidelity, surpassing current methods.",
      "chunk_index": "21-4",
      "location": {
        "page_number": 8
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "B. Comparisons with State-of-the-art Methods"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "B. Comparisons with State-of-the-art Methods"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "To evaluate the robustness of HF-VTON in multi-pose scenarios, we further analyze its performance on the SAMP-VTONS dataset, comparing single-pose (Pose 2) and multi-pose (Pose 1 + Pose 2) settings. The experiments cover both paired and unpaired settings, with results presented in Table II. The LPIPS difference between single-pose and multi-pose scenarios for HF-VTON is only  $\\Delta 0.0015$  ( $0.04 \\to 0.0415$ ), significantly lower than DCI-VTON ( $\\Delta 0.0095$ ) and MV-VTON ( $\\Delta 0.0175$ ), indicating superior texture consistency across poses. This demonstrates HF-VTON\u2019s ability to effectively handle pose variations while maintaining high texture fidelity and image quality. Additionally, the SSIM difference is reduced by 31.7% compared to DCI-VTON (2.05%), showing that HF-VTON preserves structural consistency in multi-pose scenarios, preventing reconstruction distortions or misalignment due to pose changes. These results highlight HF-VTON\u2019s robust adaptability and stability",
      "chunk_index": "22-0",
      "location": {
        "page_number": 8
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "C. Robustness Analysis"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "C. Robustness Analysis"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "structural consistency in multi-pose scenarios, preventing reconstruction distortions or misalignment due to pose changes. These results highlight HF-VTON\u2019s robust adaptability and stability under diverse poses.",
      "chunk_index": "22-1",
      "location": {
        "page_number": 8
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "C. Robustness Analysis"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "C. Robustness Analysis"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "The visual comparison in Figure 5 demonstrates the robustness of HF-VTON on the SAMP-VTONS dataset across different poses (Pose 1 and Pose 2) and garment types. The solid blue boxes highlight the person images under Pose 2, while the dashed blue boxes correspond to the results of the comparison methods under Pose 2. The results show that HF-VTON excels in geometric alignment, particularly in Pose 2, where other methods exhibit significant alignment errors in critical regions such as the shoulders and upper arms, leading to unnatural garment-body fitting. For instance, LaDI-VTON suffers from misalignment in the shoulder area, while DM-VTON and MV-VTON also fail to align the garment properly with the body. In contrast, HF-VTON achieves precise alignment between the garment and the body, especially in difficult-to-align areas like the shoulders and upper arms, ensuring smooth transitions in garment contours and pose. Moreover, HF-VTON demonstrates superior performance in handling complex",
      "chunk_index": "22-2",
      "location": {
        "page_number": 8
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "C. Robustness Analysis"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "C. Robustness Analysis"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "in difficult-to-align areas like the shoulders and upper arms, ensuring smooth transitions in garment contours and pose. Moreover, HF-VTON demonstrates superior performance in handling complex garment deformations (such as wrinkles and stretching). When dealing with wrinkled garments (e.g., the first and second rows in the figure), HF-VTON effectively",
      "chunk_index": "22-3",
      "location": {
        "page_number": 8
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "C. Robustness Analysis"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "C. Robustness Analysis"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "![Figure 4: Qualitative comparison results on the SAMP-VTONS dataset. The figure consists of three rows of images. The first row shows a maroon V-neck sweater and a person. The second row shows a black coat and a person. The third row shows a striped long-sleeve shirt and a person. For each row, there are eight columns: 'Garment', 'Person', 'PF-AFN', 'LaDI-VTON', 'DCI-VTON', 'DM-VTON', 'MV-VTON', and 'HF-VTON (Ours)'. Red boxes highlight errors in the virtual try-on results of the first seven methods, while a green box highlights the result of our method (HF-VTON).](d4e9f8f6bf5d7853ecae9c9633900af1_img.jpg)",
      "chunk_index": "23-0",
      "location": {
        "page_number": 9
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "C. Robustness Analysis"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "C. Robustness Analysis"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "Figure 4: Qualitative comparison results on the SAMP-VTONS dataset. The figure consists of three rows of images. The first row shows a maroon V-neck sweater and a person. The second row shows a black coat and a person. The third row shows a striped long-sleeve shirt and a person. For each row, there are eight columns: 'Garment', 'Person', 'PF-AFN', 'LaDI-VTON', 'DCI-VTON', 'DM-VTON', 'MV-VTON', and 'HF-VTON (Ours)'. Red boxes highlight errors in the virtual try-on results of the first seven methods, while a green box highlights the result of our method (HF-VTON).\n\nFig. 4. Qualitative comparison results on the SAMP-VTONS dataset are presented in the following columns: Garment image, Person image, and results from five state-of-the-art virtual try-on methods. Red boxes highlight errors, while green boxes emphasize the result of ours.",
      "chunk_index": "23-1",
      "location": {
        "page_number": 9
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "C. Robustness Analysis"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "C. Robustness Analysis"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "TABLE II  \nROBUSTNESS ANALYSIS OF STATE-OF-THE-ART VIRTUAL TRY-ON METHODS ON THE SAMP-VTONS DATASET WITH TWO POSES (POSE 1 CORRESPONDS TO THE SAMP-VTONS DATASET AS SHOWN IN TABLE I, AND POSE 2). THE METRICS USE  $\\uparrow/\\downarrow$  TO INDICATE THAT HIGHER/LOWER VALUES CORRESPOND TO BETTER PERFORMANCE. THE TOP TWO RESULTS ARE HIGHLIGHTED IN THE FOLLOWING ORDER: **RED** FOR THE BEST RESULT AND **GREEN** FOR THE SECOND-BEST RESULT.",
      "chunk_index": "23-2",
      "location": {
        "page_number": 9
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "C. Robustness Analysis"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "C. Robustness Analysis"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "| Method                 | Pose 2          |                    |                  |                  |                  |                  | Pose 1+Pose 2   |                    |                  |                  |                  |                  |\n|------------------------|-----------------|--------------------|------------------|------------------|------------------|------------------|-----------------|--------------------|------------------|------------------|------------------|------------------|\n|                        | Paired          |                    |                  | Unpaired         |                  |                  | Paired          |                    |                  | Unpaired         |                  |                  |\n|                        | SSIM $\\uparrow$ | LPIPS $\\downarrow$ | KID $\\downarrow$ | FID $\\downarrow$ | KID $\\downarrow$ | FID $\\downarrow$ | SSIM $\\uparrow$ | LPIPS $\\downarrow$ | KID $\\downarrow$ | FID $\\downarrow$ | KID $\\downarrow$ | FID $\\downarrow$ |\n| PF-AFN(CVPR) [14]      | 0.860           | 0.186              | 6.207            | 64.17            | 6.576            | 65.42            | 0.8515          | 0.1805             | 4.9865           | 54.045           | 5.583            | 57.365           |\n| LaDI-VTON(ACM MM) [23] | 0.895           | 0.102              | 0.449            | 8.928            | 1.348            | 18.81            | 0.8735          | 0.1015             | 0.4155           | 9.094            | 1.0595           | 15.955           |\n| DCI-VTON(ACM MM) [13]  | <b>0.927</b>    | <b>0.054</b>       | <b>0.172</b>     | <b>5.44</b>      | 0.202            | 6.31             | <b>0.906</b>    | <b>0.0635</b>      | <b>0.1805</b>    | 5.455            | <b>0.2545</b>    | <b>7.23</b>      |\n| DM-VTON(ISMAR) [50]    | 0.864           | 0.191              | 5.282            | 54.45            | 5.535            | 55.05            | 0.854           | 0.186              | 4.22             | 45.21            | 4.6195           | 47.385           |\n| MV-VTON(AAAI) [34]     | 0.890           | 0.061              | 0.200            | 5.30             | <b>0.309</b>     | <b>7.42</b>      | 0.8795          | 0.0785             | 0.21             | <b>5.7</b>       | 0.286            | 7.4              |\n| HF-VTON                | <b>0.920</b>    | <b>0.040</b>       | <b>0.082</b>     | <b>3.73</b>      | <b>0.198</b>     | <b>5.86</b>      | <b>0.906</b>    | <b>0.0415</b>      | <b>0.083</b>     | <b>3.54</b>      | <b>0.227</b>     | <b>6.39</b>      |",
      "chunk_index": 24,
      "location": {
        "page_number": 9
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "C. Robustness Analysis"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "C. Robustness Analysis",
        "type": "table"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "preserves the natural texture details of the garment, while other methods show poor texture retention, resulting in blurring or distortion.",
      "chunk_index": 25,
      "location": {
        "page_number": 9
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "C. Robustness Analysis"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "C. Robustness Analysis"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "Through ablation studies, we systematically evaluate the contributions of the APWAM, SRCM, and MPAGM modules in enhancing image quality, geometric alignment, and texture fidelity.",
      "chunk_index": "26-0",
      "location": {
        "page_number": 9
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "D. Ablation Study"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "D. Ablation Study"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "**Effectiveness of APWAM.** To evaluate the effectiveness of APWAM, we conduct comprehensive ablation experiments on the VITON-HD and SAMP-VTONS datasets, analyzing performance under different settings. The experimental setups include: (1) baseline model (BS w/o), (2) multi-scale appearance representation extraction with deformable convolution (MRE w/), (3) deformable flow estimation network (DFEN w/), and (4) deformable convolution in both MRE and DFEN (M+F w/). Each metric corresponds to two columns representing the results for the VITON-HD and SAMP-VTONS datasets, respectively. As shown in Table III, the DFEN w/ configuration outperforms the others across several metrics. In terms of SSIM, DFEN w/ achieves 0.849 and 0.872 on the two datasets, respectively, showing improvement over the BS. For the LPIPS metric, DFEN w/ DC scores 0.066 and 0.083, lower than the BS, indicating improved detail fidelity. On the KID metric,",
      "chunk_index": "26-1",
      "location": {
        "page_number": 9
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "D. Ablation Study"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "D. Ablation Study"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "DFEN w/ achieves 0.14 on SAMP-VTONS, outperforming the BS and other settings, confirming its advantage in reducing generative distribution discrepancies. Regarding FID, DFEN w/ scores 6.93 and 5.3 on VITON-HD and SAMP-VTONS, respectively, showing a significant reduction in image distortion and better alignment between garment and person. As shown in Figure 6, in the absence of deformable convolution (BS w/o), the alignment between the garment and the person is significantly insufficient, especially in critical areas such as the chest and abdomen, where noticeable misalignments degrade the naturalness of the try-on effect. With the introduction of deformable convolution into MRE and DFEN, the alignment improves notably. Particularly in the DFEN configuration, the garment\u2019s contours and details are better preserved, and the handling of wrinkle areas is more accurate.",
      "chunk_index": "26-2",
      "location": {
        "page_number": 9
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "D. Ablation Study"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "D. Ablation Study"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "**Effectiveness of SRCM.** Based on the APWAM, we further validate the effectiveness of SRCM through ablation experiments on the VITON-HD dataset with different text configurations. The experimental setups include: no text (w/o Text), raw LaDI-VTON text guidance (w/ LaDI\\_Text), and our proposed text representation based on SAS+DMTG (w/ Proposed\\_Text). The results, shown in Table IV, demonstrate that w/ Proposed\\_Text achieves the best performance across all four quantitative metrics, particularly showing significant improvements in KID and FID. Specifically, the KID for w/",
      "chunk_index": "26-3",
      "location": {
        "page_number": 9
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "D. Ablation Study"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "D. Ablation Study"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "![Figure 5: Robustness analysis results on the SAMP-VTONS dataset. The figure shows a grid of images for different methods: Garment, Person, PF-AFN, LaDI-VTON, DCI-VTON, DM-VTON, MV-VTON, and HF-VTON (Ours). Each method has two rows of images corresponding to Pose 1 and Pose 2. Blue boxes highlight the person image for Pose 2, and dashed blue boxes highlight the corresponding results from comparison methods for Pose 2.](625e10f48104ba2b06b2220a9b224712_img.jpg)\n\nFigure 5: Robustness analysis results on the SAMP-VTONS dataset. The figure shows a grid of images for different methods: Garment, Person, PF-AFN, LaDI-VTON, DCI-VTON, DM-VTON, MV-VTON, and HF-VTON (Ours). Each method has two rows of images corresponding to Pose 1 and Pose 2. Blue boxes highlight the person image for Pose 2, and dashed blue boxes highlight the corresponding results from comparison methods for Pose 2.",
      "chunk_index": "27-0",
      "location": {
        "page_number": 10
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "D. Ablation Study"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "D. Ablation Study"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "Fig. 5. Robustness analysis results on the SAMP-VTONS dataset with two poses (Pose 1 and Pose 2) are presented in the following columns: Garment image, Person image, and results from five state-of-the-art virtual try-on methods. The blue solid box highlights the person image for Pose 2, while the blue dashed box indicates the corresponding results from the comparison methods for Pose 2.\n\nTABLE III  \nABLATION STUDY ON THE PERFORMANCE OF DEFORMABLE CONVOLUTIONS (DC) IN APWAM ON THE VITON-HD AND SAMP-VTONS DATASETS: EFFECTS OF MULTI-SCALE APPEARANCE REPRESENTATION EXTRACTION (MRE) AND DEFORMABLE FLOW ESTIMATION NETWORK (DFEN). THE METRICS USE  $\\uparrow/\\downarrow$  TO INDICATE THAT LARGER/SMALLER VALUES CORRESPOND TO BETTER PERFORMANCE. THE TOP TWO RESULTS ARE HIGHLIGHTED IN THE FOLLOWING ORDER: **RED** FOR THE BEST RESULT AND **GREEN** FOR THE SECOND-BEST RESULT.",
      "chunk_index": "27-1",
      "location": {
        "page_number": 10
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "D. Ablation Study"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "D. Ablation Study"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "| Settings | SSIM $\\uparrow$ | LPIPS $\\downarrow$ | KID $\\downarrow$ | FID $\\downarrow$ |\n|----------|-----------------|--------------------|------------------|------------------|\n| BS w/o   | 0.817           | 0.101              | 0.418            | 12.31            |\n| MRE w/   | 0.848           | 0.067              | 0.075            | 6.99             |\n| DFEN w/  | <b>0.849</b>    | <b>0.066</b>       | <b>0.072</b>     | <b>6.93</b>      |\n| (M+F) w/ | <b>0.849</b>    | <b>0.067</b>       | <b>0.074</b>     | <b>6.94</b>      |",
      "chunk_index": 28,
      "location": {
        "page_number": 10
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "D. Ablation Study"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "D. Ablation Study",
        "type": "table"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "![Figure 6: Qualitative comparison of APWAM effectiveness on the VITON-HD and SAMP-VTONS datasets. The figure shows a grid of images for different settings: Garment, Person, BS w/o, MRE w/, DFEN w/, and (M+F) w/. Red boxes indicate errors, while green boxes highlight the improved results.](61ce9760bffbecba8f5e63310be1ebec_img.jpg)\n\nFigure 6: Qualitative comparison of APWAM effectiveness on the VITON-HD and SAMP-VTONS datasets. The figure shows a grid of images for different settings: Garment, Person, BS w/o, MRE w/, DFEN w/, and (M+F) w/. Red boxes indicate errors, while green boxes highlight the improved results.\n\nFig. 6. Qualitative comparison of APWAM effectiveness on the VITON-HD and SAMP-VTONS datasets. From left to right: garment images, person images, and the results from four different settings. Red boxes indicate errors, while green boxes highlight the improved results.",
      "chunk_index": "29-0",
      "location": {
        "page_number": 10
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "D. Ablation Study"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "D. Ablation Study"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "Proposed\\_Text is 0.064, which represents a 25.6% reduction compared to w/ LaDI\\_Text and an 11.1% reduction compared to w/o Text, indicating that our proposed text representation effectively reduces the generative distribution discrepancy, enhancing the consistency and stability of image generation. In terms of FID, w/ Proposed\\_Text yields a result of 6.516, which is 4.5% lower than w/ LaDI\\_Text and 6.0% lower than w/o Text, demonstrating that the SAS+DMTG-based text representation effectively reduces image distortion and improves the alignment between garment and the human body. These results highlight the significant improvement in image quality and consistency provided by the SRCM through the\n\nTABLE IV  \nQUANTITATIVE COMPARISON OF SRCM EFFECTIVENESS ON THE VITON-HD DATASET. THE METRICS USE  $\\uparrow/\\downarrow$  TO INDICATE THAT LARGER/SMALLER VALUES CORRESPOND TO BETTER PERFORMANCE. THE BEST RESULTS ARE HIGHLIGHTED IN **RED**.",
      "chunk_index": "29-1",
      "location": {
        "page_number": 10
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "D. Ablation Study"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "D. Ablation Study"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "| Settings         | SSIM $\\uparrow$ | LPIPS $\\downarrow$ | KID $\\downarrow$ | FID $\\downarrow$ |\n|------------------|-----------------|--------------------|------------------|------------------|\n| w/o Text         | 0.849           | 0.066              | 0.072            | 6.934            |\n| w/ LaDI_Text     | 0.850           | 0.061              | 0.086            | 6.826            |\n| w/ Proposed_Text | <b>0.851</b>    | <b>0.060</b>       | <b>0.064</b>     | <b>6.516</b>     |",
      "chunk_index": 30,
      "location": {
        "page_number": 10
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "D. Ablation Study"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "D. Ablation Study",
        "type": "table"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "![Figure 7: Qualitative comparison of SRCM effectiveness on the VITON-HD dataset. The figure shows a grid of images for different settings: Garment, Person, w/o Text, w/ LaDI_Text, and w/ Proposed_Text. Red boxes indicate errors, while green boxes highlight the improved results.](9e80d0382f6981fb29e6d8d55dbea732_img.jpg)\n\nFigure 7: Qualitative comparison of SRCM effectiveness on the VITON-HD dataset. The figure shows a grid of images for different settings: Garment, Person, w/o Text, w/ LaDI\\_Text, and w/ Proposed\\_Text. Red boxes indicate errors, while green boxes highlight the improved results.\n\nFig. 7. Qualitative comparison of SRCM effectiveness on the VITON-HD dataset. From left to right: garment images, person images, and results for different settings: w/o Text (baseline), w/ LaDI\\_Text (raw text), and w/ Proposed\\_Text (SAS+DMTG). Red boxes indicate errors, while green boxes highlight the improved results.\n\nproposed text representation.",
      "chunk_index": "31-0",
      "location": {
        "page_number": 10
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "D. Ablation Study"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "D. Ablation Study"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "proposed text representation.\n\nThe visual results in Figure 7 demonstrate the effectiveness of the SRCM framework with different text settings. In the baseline (w/o Text), misalignments between the garment and body are evident, particularly around the shoulder and chest, resulting in an unnatural fit. Using raw LaDI\\_Text (w/ LaDI\\_Text) shows some improvement, but artifacts remain, especially around the garment edges. In contrast, the proposed text representation (w/ Proposed\\_Text) significantly improves alignment, especially in the shoulder and chest regions, as highlighted by the green boxes. These visual improvements are consistent with the quantitative results, where our method outperforms others in reducing image distortion and enhancing garment-body consistency, demonstrating the effectiveness of our proposed representation in improving both visual quality and fit accuracy.",
      "chunk_index": "31-1",
      "location": {
        "page_number": 10
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "D. Ablation Study"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "D. Ablation Study"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "**Effectiveness of MPAGM.** Building upon the APWAM and SRCM, we further validate the effectiveness of MPAGM through an ablation study on the VITON-HD dataset with",
      "chunk_index": "31-2",
      "location": {
        "page_number": 10
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "D. Ablation Study"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "D. Ablation Study"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "TABLE V  \nQUANTITATIVE COMPARISON OF MPAGM EFFECTIVENESS ON THE VITON-HD DATASET. THE METRICS USE  $\\uparrow/\\downarrow$  TO INDICATE THAT LARGER/SMALLER VALUES CORRESPOND TO BETTER PERFORMANCE. THE BEST RESULTS ARE HIGHLIGHTED IN **RED**.",
      "chunk_index": 32,
      "location": {
        "page_number": 11
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "D. Ablation Study"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "D. Ablation Study"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "| Settings                     | SSIM $\\uparrow$ | LPIPS $\\downarrow$ | KID $\\downarrow$ | FID $\\downarrow$ |\n|------------------------------|-----------------|--------------------|------------------|------------------|\n| only w/ APWAM                | 0.849           | 0.066              | 0.072            | 6.83             |\n| only w/ SRCM                 | 0.847           | 0.053              | 0.036            | 5.72             |\n| APWAM + SRCM (LaDI_Text)     | 0.851           | 0.050              | 0.027            | 5.57             |\n| APWAM + SRCM (Proposed_Text) | <b>0.852</b>    | <b>0.050</b>       | <b>0.024</b>     | <b>5.51</b>      |",
      "chunk_index": 33,
      "location": {
        "page_number": 11
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "D. Ablation Study"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "D. Ablation Study",
        "type": "table"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "![Figure 8: Qualitative comparison of MPAGM effectiveness on the VITON-HD dataset. The figure shows two rows of images. The top row shows a woman wearing a black t-shirt and a white skirt. The bottom row shows a man wearing a white t-shirt with a 'POLO' logo. Each row has six columns: (a) garment image, (b) person image, and four results from different settings. Red boxes highlight misalignments in the shoulder and chest areas for settings (c) and (d). Green boxes highlight improved alignment and texture quality for settings (e) and (f).](3668a836db39d25d24b56180a9c9a7fb_img.jpg)",
      "chunk_index": "34-0",
      "location": {
        "page_number": 11
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "D. Ablation Study"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "D. Ablation Study"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "Figure 8: Qualitative comparison of MPAGM effectiveness on the VITON-HD dataset. The figure shows two rows of images. The top row shows a woman wearing a black t-shirt and a white skirt. The bottom row shows a man wearing a white t-shirt with a 'POLO' logo. Each row has six columns: (a) garment image, (b) person image, and four results from different settings. Red boxes highlight misalignments in the shoulder and chest areas for settings (c) and (d). Green boxes highlight improved alignment and texture quality for settings (e) and (f).\n\nFig. 8. Qualitative comparison of MPAGM effectiveness on the VITON-HD dataset. From left to right: garment image, person image, and results of four settings\u2014only w/ APWAM, only w/ SRCM, APWAM + SRCM (LaDI\\_Text), and APWAM + SRCM (Proposed\\_Text). Red boxes indicate errors, while green boxes highlight the improved results.",
      "chunk_index": "34-1",
      "location": {
        "page_number": 11
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "D. Ablation Study"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "D. Ablation Study"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "different settings. The settings include: only APWAM, only SRCM, APWAM + SRCM (LaDI\\_Text), and APWAM + SRCM (Proposed\\_Text). As shown in Table V, APWAM + SRCM (Proposed\\_Text) outperforms other configurations, particularly in KID and FID. Specifically, KID is reduced to 0.024, showing a 66.7% decrease compared to only APWAM (0.072) and 33.3% compared to only SRCM (0.036). For FID, it achieves 5.51, a 19.4% reduction from only APWAM (6.83) and 3.7% from only SRCM (5.72). These results demonstrate MPAGM\u2019s effectiveness in reducing generative distribution discrepancies and image distortion, leading to better garment-person alignment.",
      "chunk_index": "34-2",
      "location": {
        "page_number": 11
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "D. Ablation Study"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "D. Ablation Study"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "Figure 8 illustrates the impact of different configurations on garment-body alignment and texture fidelity. In the configurations using only APWAM (c) or only SRCM (d), clear misalignments are observed, particularly in the shoulder region, along with noticeable distortions in garment textures such as logos. When APWAM and SRCM are combined (e), the alignment improves, especially around the shoulder and chest areas, resulting in more natural fitting, though artifacts persist near the hemline. With the proposed text representation (f), both alignment and texture quality are further enhanced. The regions highlighted in green exhibit more accurate and natural fitting, confirming the effectiveness of our method in improving overall image quality and structural consistency.",
      "chunk_index": "34-3",
      "location": {
        "page_number": 11
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "IV. EXPERIMENTS"
        },
        {
          "level": "Header 3",
          "name": "D. Ablation Study"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "IV. EXPERIMENTS",
        "Header 3": "D. Ablation Study"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "This paper introduces a comprehensive framework for high-fidelity virtual try-on, addressing key challenges in geometric, semantic, and fine-grained consistency. The proposed approach integrates multiple modules to ensure accurate garment fitting and realistic visual rendering across diverse poses. A central insight of this work is the effective management of consistency. The Appearance-Preserving Warp Alignment Module (APWAM) uses a multi-scale architecture to capture both local and global geometric features, ensuring spatial alignment\n\nwhile preserving fine-grained garment details, such as wrinkles and material deformation. This capability is essential for improving visual accuracy, particularly when handling complex fabric deformations.",
      "chunk_index": "35-0",
      "location": {
        "page_number": 11
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "V. DISCUSSION"
        },
        {
          "level": "Header 3",
          "name": "A. Insights on Consistency"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "V. DISCUSSION",
        "Header 3": "A. Insights on Consistency"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "The Semantic Representation and Comprehension Module (SRCM) enhances semantic consistency by extending traditional garment attribute structures with fine-grained features. It refines garment descriptions by integrating multi-pose datasets, ensuring precise semantic alignment between garments and human body representations across poses. This integration not only improves garment fitting consistency but also facilitates the generation of accurate textual descriptions of garments, further supporting the alignment between visual and semantic representations.\n\nFinally, the Multimodal Prior-Guided Appearance Generation Module (MPAGM) combines geometric priors with semantic and visual cues to optimize the virtual try-on process. This multimodal integration ensures the generation of realistic images that preserve both garment structure and appearance, maintaining consistency across a wide range of garment types and poses.",
      "chunk_index": "35-1",
      "location": {
        "page_number": 11
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "V. DISCUSSION"
        },
        {
          "level": "Header 3",
          "name": "A. Insights on Consistency"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "V. DISCUSSION",
        "Header 3": "A. Insights on Consistency"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "Despite significant progress, several limitations remain, primarily related to consistency across varying poses and garment types. One major challenge is the model\u2019s performance in handling extreme pose variations and intricate garment textures. While the framework performs well with standard poses and common garment types, it struggles to accurately model complex, non-rigid fabric deformations and rare garment styles. Future work should focus on improving the model\u2019s robustness to handle more complex and diverse garment deformations while maintaining consistency in alignment across a broader range of garment types. Another limitation stems from the framework\u2019s dependence on large-scale labeled datasets and precise pose estimation. While high-quality pose information and extensive labeled data are crucial for optimal performance, such resources may not be readily available in real-world scenarios. To mitigate this, future research could explore unsupervised or semi-supervised learning",
      "chunk_index": "36-0",
      "location": {
        "page_number": 11
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "V. DISCUSSION"
        },
        {
          "level": "Header 3",
          "name": "B. Limitations and Future Directions"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "V. DISCUSSION",
        "Header 3": "B. Limitations and Future Directions"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "data are crucial for optimal performance, such resources may not be readily available in real-world scenarios. To mitigate this, future research could explore unsupervised or semi-supervised learning techniques to reduce reliance on labeled data and improve the model\u2019s generalization ability. Additionally, although the framework demonstrates promising results, computational efficiency remains a critical challenge, particularly for real-time applications. Future efforts should focus on optimizing the framework to achieve faster inference times without compromising consistency or accuracy. Techniques such as model compression, distillation, and hardware acceleration could be investigated to enhance processing speed, enabling real-world deployment.",
      "chunk_index": "36-1",
      "location": {
        "page_number": 11
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "V. DISCUSSION"
        },
        {
          "level": "Header 3",
          "name": "B. Limitations and Future Directions"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "V. DISCUSSION",
        "Header 3": "B. Limitations and Future Directions"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "Looking ahead, several promising directions for future research include extending the framework to other garment categories, such as accessories, footwear, and outerwear, where unique characteristics may require specialized consistency handling. Moreover, integrating additional sensor modalities, such as depth sensing or multi-view cameras, could offer richer data for more accurate garment fitting and pose alignment. Finally, improving real-time performance and reducing computational overhead will be key to the broader adoption of virtual try-on systems in commercial applications.",
      "chunk_index": "36-2",
      "location": {
        "page_number": 11
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "V. DISCUSSION"
        },
        {
          "level": "Header 3",
          "name": "B. Limitations and Future Directions"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "V. DISCUSSION",
        "Header 3": "B. Limitations and Future Directions"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "This paper presents a high-fidelity virtual try-on framework that focuses on achieving consistency in geometric alignment, appearance, and semantic understanding of garments.",
      "chunk_index": 37,
      "location": {
        "page_number": 11
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "VI. CONCLUSION"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "VI. CONCLUSION"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "By integrating the Appearance-Preserving Warp Alignment Module (APWAM), the Semantic Representation and Comprehensive Module (SRCM), and the Multimodal Prior-Guided Appearance Generation Module (MPAGM), the framework addresses key challenges in preserving geometric consistency across varying poses, ensuring semantic consistency between garments and body poses, and retaining fine-grained garment details. Extensive experiments demonstrate that the proposed framework significantly outperforms existing methods in terms of visual quality, semantic alignment, and garment fitting accuracy, showcasing its capability to handle complex garment deformations and generate realistic virtual try-on images. However, challenges remain in handling extreme poses and highly intricate garment structures. Future work will focus on improving robustness, enhancing computational efficiency, and extending the framework to a broader range of garment types. Overall, the proposed framework establishes a new",
      "chunk_index": "38-0",
      "location": {
        "page_number": 12
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "VI. CONCLUSION"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "VI. CONCLUSION"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "Future work will focus on improving robustness, enhancing computational efficiency, and extending the framework to a broader range of garment types. Overall, the proposed framework establishes a new standard for virtual try-on technology, offering a solid foundation for future research and real-world applications, particularly in enhancing consistency across diverse poses and garment types.",
      "chunk_index": "38-1",
      "location": {
        "page_number": 12
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "VI. CONCLUSION"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "VI. CONCLUSION"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "- [1] S. Choi, S. Park, M. Lee, and J. Choo, \"VITON-HD: High-resolution virtual try-on via misalignment-aware normalization,\" in *Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)*, 2021, pp. 14131\u201314140.\n- [2] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, \"Image quality assessment: from error visibility to structural similarity,\" *IEEE Trans. Image Process.*, vol. 13, no. 4, pp. 600\u2013612, Apr. 2004.\n- [3] T. Wei *et al.*, \"E2Style: Improve the efficiency and effectiveness of StyleGAN inversion,\" *IEEE Trans. Image Process.*, vol. 31, pp. 3267\u20133280, 2022.\n- [4] Q. Mao *et al.*, \"Scalable face image coding via stylegan prior: Toward compression for human-machine collaborative vision,\" *IEEE Trans. Image Process.*, vol. 33, pp. 408\u2013422, 2024.\n- [5] T. Liu *et al.*, \"Spatial-aware texture transformer for high-fidelity garment transfer,\" *IEEE Trans. Image Process.*, vol. 30, pp. 7499\u20137510, 2021.",
      "chunk_index": "39-0",
      "location": {
        "page_number": 12
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "REFERENCES"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "REFERENCES"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "- [5] T. Liu *et al.*, \"Spatial-aware texture transformer for high-fidelity garment transfer,\" *IEEE Trans. Image Process.*, vol. 30, pp. 7499\u20137510, 2021.\n- [6] Y. Shi, Y. Liu, J. Cheng, Z. J. Wang and X. Chen, \"VDMUFusion: A versatile diffusion model-based unsupervised framework for image fusion,\" *IEEE Trans. Image Process.*, vol. 34, pp. 441\u2013454, 2025.\n- [7] C. -Y. Chan, W. -C. Siu, Y. -H. Chan and H. Anthony Chan, \"An-lightenDiff: Anchoring diffusion probabilistic model on low light image enhancement,\" *IEEE Trans. Image Process.*, vol. 33, pp. 6324\u20136339, 2024.\n- [8] J. Yue, L. Fang, S. Xia, Y. Deng and J. Ma, \"Dif-fusion: Toward high color fidelity in infrared and visible image fusion with diffusion models,\" *IEEE Trans. Image Process.*, vol. 32, pp. 5705\u20135720, 2023.",
      "chunk_index": "39-1",
      "location": {
        "page_number": 12
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "REFERENCES"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "REFERENCES"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "- [9] D. Morelli, M. Fincato, M. Cornia, F. Landi, F. Cesari, and R. Cucchiara, \"Dress code: High-resolution multi-category virtual try-on,\" in *Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)*, 2022, pp. 2231\u20132235.\n- [10] H.-W. Jin and D.-O. Kang, \"Versatile-VTON: A versatile virtual try-on network,\" in *Proc. IEEE Int. Conf. Consumer Electron.-Asia (ICCE-Asia)*, Busan, South Korea, 2023, pp. 1\u20134.\n- [11] N. Zheng, X. Song, Z. Chen, L. Hu, D. Cao, and L. Nie, \"Virtually trying on new clothing with arbitrary poses,\" in *Proc. 27th ACM Int. Conf. Multimedia (MM)*, Nice, France, 2019, pp. 266\u2013274.\n- [12] X. Han, X. Hu, W. Huang, and M. R. Scott, \"ClothFlow: A flow-based model for clothed person generation,\" in *Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV)*, 2019, pp. 10471\u201310480.",
      "chunk_index": "39-2",
      "location": {
        "page_number": 12
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "REFERENCES"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "REFERENCES"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "- [12] X. Han, X. Hu, W. Huang, and M. R. Scott, \"ClothFlow: A flow-based model for clothed person generation,\" in *Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV)*, 2019, pp. 10471\u201310480.\n- [13] J. Gou, S. Sun, J. Zhang, J. Si, C. Qian, and L. Zhang, \"Taming the power of diffusion models for high-quality virtual try-on with appearance flow,\" in *Proc. 31st ACM Int. Conf. Multimedia*, 2023, pp. 7599\u20137607.\n- [14] Y. Ge, Y. Song, R. Zhang, C. Ge, W. Liu, and P. Luo, \"Parser-free virtual try-on via distilling appearance flows,\" in *Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)*, 2021, pp. 8485\u20138493.\n- [15] J. Dai *et al.*, \"Deformable convolutional networks,\" in *Proc. IEEE Int. Conf. Comput. Vis. (ICCV)*, 2017, pp. 764\u2013773.\n- [16] T. Xu *et al.*, \"AttnGAN: Fine-grained text to image generation with attentional generative adversarial networks,\" in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)*, 2018, pp. 1316\u20131324.",
      "chunk_index": "39-3",
      "location": {
        "page_number": 12
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "REFERENCES"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "REFERENCES"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "- [17] M. Zhu, P. Pan, W. Chen, and Y. Yang, \"DM-GAN: Dynamic memory generative adversarial networks for text-to-image synthesis,\" in *Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)*, 2019, pp. 5802\u20135810.\n- [18] H. Zhang, J. Y. Koh, J. Baldridge, H. Lee, and Y. Yang, \"Cross-modal contrastive learning for text-to-image generation,\" in *Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)*, 2021, pp. 833\u2013842.\n- [19] M. Tao, H. Tang, F. Wu, X.-Y. Jing, B.-K. Bao, and C. Xu, \"DF-GAN: A simple and effective baseline for text-to-image synthesis,\" in *Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)*, 2022, pp. 16515\u201316525.\n- [20] A. Ramesh *et al.*, \"Zero-shot text-to-image generation,\" in *Proc. 38th Int. Conf. Mach. Learn. (ICML)*, PMLR, Jul. 2021, pp. 8821\u20138831.\n- [21] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, \"Hierarchical text-conditional image generation with CLIP latents,\" 2022, *arXiv:2204.06125*.",
      "chunk_index": "39-4",
      "location": {
        "page_number": 12
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "REFERENCES"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "REFERENCES"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "- [21] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, \"Hierarchical text-conditional image generation with CLIP latents,\" 2022, *arXiv:2204.06125*.\n- [22] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee, \"Generative adversarial text to image synthesis,\" in *Proc. 33rd Int. Conf. Mach. Learn.*, PMLR, Jun. 2016, pp. 1060\u20131069.\n- [23] D. Morelli, A. Baldrati, G. Cartella, M. Cornia, M. Bertini, and R. Cucchiara, \"LaDI-VTON: Latent diffusion textual-inversion enhanced virtual try-on,\" in *Proc. 31st ACM Int. Conf. Multimedia*, 2023, pp. 8580\u20138589.\n- [24] R. Wang *et al.*, \"StableGarment: Garment-centric generation via stable diffusion,\" 2024, *arXiv:2403.10783*.\n- [25] Y. Xu, T. Gu, W. Chen, and A. Chen, \"OOTDiffusion: Outfitting fusion based latent diffusion for controllable virtual try-on,\" *Proc. AAAI Conf. Artif. Intell.*, vol. 39, no. 9, Art. no. 9, Apr. 2025.",
      "chunk_index": "39-5",
      "location": {
        "page_number": 12
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "REFERENCES"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "REFERENCES"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "- [26] M. S. Seyfioglu, K. Bouyarmane, S. Kumar, A. Tavanaei, and I. B. Tutar, \"Diffuse to choose: Enriching image conditioned inpainting in latent diffusion models for virtual try-all,\" 2024, *arXiv:2401.13795*.\n- [27] A. Q. Nichol and P. Dhariwal, \"Improved denoising diffusion probabilistic models,\" in *Proc. 38th Int. Conf. Mach. Learn.*, PMLR, Jul. 2021, pp. 8162\u20138171.\n- [28] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli, \"Deep unsupervised learning using nonequilibrium thermodynamics,\" in *Proc. Int. Conf. Mach. Learn.*, PMLR, Jun. 2015, pp. 2256\u20132265.\n- [29] L. Zhu *et al.*, \"TryOnDiffusion: A tale of two unets,\" in *Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)*, 2023, pp. 4606\u20134615.\n- [30] R. Gal *et al.*, \"An image is worth one word: Personalizing text-to-image generation using textual inversion,\" 2022, *arXiv:2208.01618*.",
      "chunk_index": "39-6",
      "location": {
        "page_number": 12
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "REFERENCES"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "REFERENCES"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "- [30] R. Gal *et al.*, \"An image is worth one word: Personalizing text-to-image generation using textual inversion,\" 2022, *arXiv:2208.01618*.\n- [31] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, \"Language models are unsupervised multitask learners,\" *OpenAI Blog*, vol. 1, 8, pp. 9, 2019.\n- [32] I. Han, S. Yang, T. Kwon, and J. C. Ye, \"Highly personalized text embedding for image manipulation by stable diffusion,\" 2023, *arXiv:2303.08767*.\n- [33] A. Radford *et al.*, \"Learning transferable visual models from natural language supervision,\" in *Proc. 38th Int. Conf. Mach. Learn. (ICML)*, PMLR, Jul. 2021, pp. 8748\u20138763.\n- [34] H. Wang, Z. Zhang, D. Di, S. Zhang, and W. Zuo, \"MV-VTON: Multi-view virtual try-on with diffusion models,\" *Proc. AAAI Conf. Artif. Intell.*, vol. 39, no. 7, Art. no. 7, Apr. 2025.",
      "chunk_index": "39-7",
      "location": {
        "page_number": 12
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "REFERENCES"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "REFERENCES"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "- [34] H. Wang, Z. Zhang, D. Di, S. Zhang, and W. Zuo, \"MV-VTON: Multi-view virtual try-on with diffusion models,\" *Proc. AAAI Conf. Artif. Intell.*, vol. 39, no. 7, Art. no. 7, Apr. 2025.\n- [35] B. Wang, H. Zheng, X. Liang, Y. Chen, L. Lin, and M. Yang, \"Toward characteristic-preserving image-based virtual try-on network,\" in *Proc. Eur. Conf. Comput. Vis. (ECCV)*, 2018, pp. 589\u2013604.\n- [36] J. Kim, G. Gu, M. Park, S. Park, and J. Choo, \"StableVITON: Learning semantic correspondence with latent diffusion model for virtual try-on,\" in *Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)*, 2024, pp. 8176\u20138185.\n- [37] L. Zhang, A. Rao, and M. Agrawala, \"Adding conditional control to text-to-image diffusion models,\" in *Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV)*, 2023, pp. 3836\u20133847.\n- [38] J. Johnson, A. Alahi, and L. Fei-Fei, \"Perceptual losses for real-time style transfer and super-resolution,\" in *Computer Vision \u2013 ECCV 2016*, 2016, pp. 694\u2013711.",
      "chunk_index": "39-8",
      "location": {
        "page_number": 12
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "REFERENCES"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "REFERENCES"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "- [38] J. Johnson, A. Alahi, and L. Fei-Fei, \"Perceptual losses for real-time style transfer and super-resolution,\" in *Computer Vision \u2013 ECCV 2016*, 2016, pp. 694\u2013711.\n- [39] K. Simonyan and A. Zisserman, \"Very deep convolutional networks for large-scale image recognition,\" 2015, *arXiv:1409.1556*.\n- [40] J. Deng, W. Dong, R. Socher, L. -J. Li, K. Li, and L. Fei-Fei, \"ImageNet: A large-scale hierarchical image database,\" *IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)*, Miami, FL, USA, 2009, pp. 248\u2013255.\n- [41] D. Sun, S. Roth, and M. J. Black, \"A quantitative analysis of current practices in optical flow estimation and the principles behind them,\" *Int. J. Comput. Vis. (IJCV)*, vol. 106, no. 2, pp. 115\u2013137, Jan. 2014.\n- [42] Alibaba Cloud Tianchi, 2022, \"Tianchi FashionAI Dataset,\" [Online]. Available: <https://tianchi.aliyun.com/dataset/136948>.",
      "chunk_index": "39-9",
      "location": {
        "page_number": 12
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "REFERENCES"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "REFERENCES"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "- [42] Alibaba Cloud Tianchi, 2022, \"Tianchi FashionAI Dataset,\" [Online]. Available: <https://tianchi.aliyun.com/dataset/136948>.\n- [43] R. A. G\u00fcler, N. Neverova, and I. Kokkinos, \"Densepose: Dense human pose estimation in the wild,\" in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)*, 2018, pp. 7297\u20137306.\n- [44] Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh, \"Realtime multi-person 2d pose estimation using part affinity fields,\" in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)*, 2017, pp. 7291\u20137299.\n- [45] P. Li, Y. Xu, Y. Wei, and Y. Yang, \"Self-correction for human parsing,\" *IEEE Trans. Pattern Anal. Mach. Intell.*, vol. 44, no. 6, pp. 3260\u20133271, Jun. 2022.\n- [46] X. Han, Z. Wu, Z. Wu, R. Yu, and L. S. Davis, \"VITON: An image-based virtual try-on network,\" in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)*, 2018, pp. 7543\u20137552.\n- [47] D. P. Kingma and J. Ba, \"Adam: A method for stochastic optimization,\" 2014, *arXiv:1412.6980*.",
      "chunk_index": "39-10",
      "location": {
        "page_number": 12
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "REFERENCES"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "REFERENCES"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "[48] M. Wortsman *et al.*, \u201cRobust fine-tuning of zero-shot models,\u201d in *Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)*, 2022, pp. 7959\u20137971.\n\n[49] C. Schuhmann *et al.*, \u201cLAION-5B: An open large-scale dataset for training next generation image-text models,\u201d *Adv. Neural Inf. Process. Syst.*, vol. 35, pp. 25278\u201325294, Dec. 2022.\n\n[50] K.-N. Nguyen-Ngoc, T.-T. Phan-Nguyen, K.-D. Le, T. V. Nguyen, M.-T. Tran, and T.-N. Le, \u201cDM-VTON: Distilled mobile real-time virtual try-on,\u201d in *Proc. IEEE Int. Symp. Mixed Augmented Reality Adjunct (ISMAR-Adjunct)*, Sydney, Australia, 2023, pp. 695\u2013700.\n\n[51] J. Ho and T. Salimans, \u201cClassifier-free diffusion guidance,\u201d 2022, *arXiv:2207.12598*.\n\n[52] O. Avrahami *et al.*, \u201cSpaText: Spatio-textual representation for controllable image generation,\u201d in *Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)*, 2023, pp. 18370\u201318380.",
      "chunk_index": "40-0",
      "location": {
        "page_number": 13
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "REFERENCES"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "REFERENCES"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "[52] O. Avrahami *et al.*, \u201cSpaText: Spatio-textual representation for controllable image generation,\u201d in *Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)*, 2023, pp. 18370\u201318380.\n\n[53] T. Karras, S. Laine, and T. Aila, \u201cA style-based generator architecture for generative adversarial networks,\u201d in *Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)*, 2019, pp. 4401\u20134410.\n\n[54] N. Jetchev and U. Bergmann, \u201cThe conditional analogy GAN: Swapping fashion articles on people images,\u201d in *Proc. IEEE Int. Conf. Comput. Vis. Workshops (ICCVW)*, 2017, pp. 2287\u20132292.\n\n[55] R. Yu, X. Wang, and X. Xie, \u201cVTNFP: An image-based virtual try-on network with body and clothing feature preservation,\u201d in *Proc. IEEE/CVF Int. Conf. Comput. Vis.*, 2019, pp. 10511\u201310520.\n\n[56] H. J. Lee, R. Lee, M. Kang, M. Cho, and G. Park, \u201cLA-VITON: A network for looking-attractive virtual try-on,\u201d in *Proc. IEEE/CVF Int. Conf. Comput. Vis. Workshops*, 2019.",
      "chunk_index": "40-1",
      "location": {
        "page_number": 13
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "REFERENCES"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "REFERENCES"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "[56] H. J. Lee, R. Lee, M. Kang, M. Cho, and G. Park, \u201cLA-VITON: A network for looking-attractive virtual try-on,\u201d in *Proc. IEEE/CVF Int. Conf. Comput. Vis. Workshops*, 2019.\n\n[57] H. Yang, R. Zhang, X. Guo, W. Liu, W. Zuo, and P. Luo, \u201cTowards photo-realistic virtual try-on by adaptively generating-preserving image content,\u201d in *Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)*, 2020, pp. 7850\u20137859.\n\n[58] Z. Xie *et al.*, \u201cGP-VTON: Towards general purpose virtual try-on via collaborative local-flow global-parsing learning,\u201d in *Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)*, 2023, pp. 23550\u201323559.\n\n[59] O. Ronneberger, P. Fischer, and T. Brox, \u201cU-Net: Convolutional networks for biomedical image segmentation,\u201d in *Medical Image Computing and Computer-Assisted Intervention \u2013 MICCAI 2015*, 2015, pp. 234\u2013241.",
      "chunk_index": "40-2",
      "location": {
        "page_number": 13
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "REFERENCES"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "REFERENCES"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "[60] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, \u201cThe unreasonable effectiveness of deep features as a perceptual metric,\u201d in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)*, 2018, pp. 586\u2013595.\n\n[61] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, \u201cGANs trained by a two time-scale update rule converge to a local Nash equilibrium,\u201d in *Advances in Neural Information Processing Systems*, vol. 30, pp. 6626\u20136637, 2017.\n\n[62] M. Bi\u0144kowski, D. J. Sutherland, M. Arbel, and A. Gretton, \u201cDemystifying MMD GANs,\u201d 2021, *arXiv:1801.01401*.\n\n[63] I. J. Goodfellow *et al.*, \u201cGenerative adversarial nets,\u201d in *Advances in Neural Information Processing Systems*, Curran Associates, Inc., 2014.\n\n[64] J. Ho, A. Jain, and P. Abbeel, \u201cDenoising diffusion probabilistic models,\u201d in *Advances in Neural Information Processing Systems*, Curran Associates, Inc., 2020, pp. 6840\u20136851.",
      "chunk_index": "40-3",
      "location": {
        "page_number": 13
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "REFERENCES"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "REFERENCES"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "[64] J. Ho, A. Jain, and P. Abbeel, \u201cDenoising diffusion probabilistic models,\u201d in *Advances in Neural Information Processing Systems*, Curran Associates, Inc., 2020, pp. 6840\u20136851.\n\n[65] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, \u201cHigh-resolution image synthesis with latent diffusion models,\u201d in *Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR)*, 2022, pp. 10684\u201310695.\n\n![Portrait of Jiajie Li, an undergraduate student at Data Science and Media Intelligence college, Communication University of China, China. Her current research interest is computer vision.](bb6d33498937738ff5dac8d91c9ebaad_img.jpg)\n\nPortrait of Jiajie Li, an undergraduate student at Data Science and Media Intelligence college, Communication University of China, China. Her current research interest is computer vision.",
      "chunk_index": "40-4",
      "location": {
        "page_number": 13
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "REFERENCES"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "REFERENCES"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "Portrait of Jiajie Li, an undergraduate student at Data Science and Media Intelligence college, Communication University of China, China. Her current research interest is computer vision.\n\n**Jiajie Li** is an undergraduate student at Data Science and Media Intelligence college, Communication University of China, China. Her current research interest is computer vision.\n\n![Portrait of Zhe Zhu, a Research Engineer at Samsung Research America. He received his Ph.D. from the Department of Computer Science and Technology, Tsinghua University in 2017. He received his bachelor's degree from Wuhan University in 2011. His research interests are in computer vision and computer graphics.](c092f712a80ce3310c5e29d0fa0e454a_img.jpg)",
      "chunk_index": "40-5",
      "location": {
        "page_number": 13
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "REFERENCES"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "REFERENCES"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "Portrait of Zhe Zhu, a Research Engineer at Samsung Research America. He received his Ph.D. from the Department of Computer Science and Technology, Tsinghua University in 2017. He received his bachelor's degree from Wuhan University in 2011. His research interests are in computer vision and computer graphics.\n\n**Zhe Zhu** is currently a Research Engineer at Samsung Research America. He got his Ph.D. from the Department of Computer Science and Technology, Tsinghua University in 2017. He received his bachelor's degree from Wuhan University in 2011. His research interests are in computer vision and computer graphics.",
      "chunk_index": "40-6",
      "location": {
        "page_number": 13
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "REFERENCES"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "REFERENCES"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "![Portrait of Wang Xinyu, from the School of Artificial Intelligence, Beihang University. His field is the sparsification and quantization of large-model inference. He aims to boost inference efficiency and cut computational costs. He is keen on quantitative trading algorithms, using AI to build trading strategies and analyze financial data for investment chances. He also focuses on AI education, exploring ways to enhance education with AI.](b8205e5e617a8946ddc956c816156fec_img.jpg)\n\nPortrait of Wang Xinyu, from the School of Artificial Intelligence, Beihang University. His field is the sparsification and quantization of large-model inference. He aims to boost inference efficiency and cut computational costs. He is keen on quantitative trading algorithms, using AI to build trading strategies and analyze financial data for investment chances. He also focuses on AI education, exploring ways to enhance education with AI.",
      "chunk_index": "40-7",
      "location": {
        "page_number": 13
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "REFERENCES"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "REFERENCES"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "**Wang Xinyu** is from the School of Artificial Intelligence, Beihang University. My field is the sparsification and quantization of large-model inference. By optimizing model structure and parameter storage, I aim to boost inference efficiency and cut computational costs. I'm keen on quantitative trading algorithms, using AI to build trading strategies and analyze financial data for investment chances. Also, I focus on AI education, exploring ways to enhance education with AI.\n\n![Portrait of Zhaoxin Fan, who received his Ph.D. degree from the School of Information, Renmin University, China in 2024. He has also conducted research at Carnegie Mellon University and Hong Kong University of Science and Technology. He is currently an Assistant Researcher in the Institute of Artificial Intelligence, Beihang University. His research interests include multi-modal large language models (LLMs), computer vision, and embodied AI.](34fccc54a5930cbdf0f07e02c3745e35_img.jpg)",
      "chunk_index": "40-8",
      "location": {
        "page_number": 13
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "REFERENCES"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "REFERENCES"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "Portrait of Zhaoxin Fan, who received his Ph.D. degree from the School of Information, Renmin University, China in 2024. He has also conducted research at Carnegie Mellon University and Hong Kong University of Science and Technology. He is currently an Assistant Researcher in the Institute of Artificial Intelligence, Beihang University. His research interests include multi-modal large language models (LLMs), computer vision, and embodied AI.\n\n**Zhaoxin Fan** received his Ph.D. degree from the School of Information, Renmin University, China in 2024. He has also conducted research at Carnegie Mellon University and Hong Kong University of Science and Technology. He is currently an Assistant Researcher in the Institute of Artificial Intelligence, Beihang University. His research interests include multi-modal large language models (LLMs), computer vision, and embodied AI.",
      "chunk_index": "40-9",
      "location": {
        "page_number": 13
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "REFERENCES"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "REFERENCES"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "![Portrait of Wei Zhao, an associate professor at Communication University of China (CUC). She received her PhD degree from the Communication and Information System major at CUC. Her main research interests include intelligent audio video processing, virtual reality technology, Accessible Communication and Interaction Technologies.](832a3f95cc4469630aca33d36af8602c_img.jpg)\n\nPortrait of Wei Zhao, an associate professor at Communication University of China (CUC). She received her PhD degree from the Communication and Information System major at CUC. Her main research interests include intelligent audio video processing, virtual reality technology, Accessible Communication and Interaction Technologies.",
      "chunk_index": "40-10",
      "location": {
        "page_number": 13
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "REFERENCES"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "REFERENCES"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "**Wei Zhao** is currently an associate professor at Communication University of China (CUC). She received her PhD degree from the Communication and Information System major at CUC. Her main research interests include intelligent audio video processing, virtual reality technology, Accessible Communication and Interaction Technologies.\n\n![Portrait of Ming Meng, a lecturer at the Communications University of China. She received her Ph.D. degree from the State Key Laboratory of Virtual Reality Technology and Systems, Beihang University in 2022. Her main research interests include mixed reality (MR), computer vision, and computer graphics, with a particular focus on scene structure recovery and modeling and driving of virtual digital humans.](08e28a86c71221f509c8f3a828e89af2_img.jpg)",
      "chunk_index": "40-11",
      "location": {
        "page_number": 13
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "REFERENCES"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "REFERENCES"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "Portrait of Ming Meng, a lecturer at the Communications University of China. She received her Ph.D. degree from the State Key Laboratory of Virtual Reality Technology and Systems, Beihang University in 2022. Her main research interests include mixed reality (MR), computer vision, and computer graphics, with a particular focus on scene structure recovery and modeling and driving of virtual digital humans.\n\n**Ming Meng** is currently a lecturer at the Communications University of China. She received her Ph.D. degree from the State Key Laboratory of Virtual Reality Technology and Systems, Beihang University in 2022. Her main research interests include mixed reality (MR), computer vision, and computer graphics, with a particular focus on scene structure recovery and modeling and driving of virtual digital humans.",
      "chunk_index": "40-12",
      "location": {
        "page_number": 13
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "REFERENCES"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "REFERENCES"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "![Portrait of Wenjun Wu, who received the PhD degree in computer science from Beihang University in 2001. He was employed with Argonne National Laboratory as a research scientist working on grid computing, cloud computing, media collaboration, etc., until 2012. He is currently employed with Beihang University as a professor. His research interests include crowdsourcing, machine learning, cloud computing, eScience, and cyber infrastructure.](148e42a70f7b9fcc3db671c0a2a021a1_img.jpg)\n\nPortrait of Wenjun Wu, who received the PhD degree in computer science from Beihang University in 2001. He was employed with Argonne National Laboratory as a research scientist working on grid computing, cloud computing, media collaboration, etc., until 2012. He is currently employed with Beihang University as a professor. His research interests include crowdsourcing, machine learning, cloud computing, eScience, and cyber infrastructure.",
      "chunk_index": "40-13",
      "location": {
        "page_number": 13
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "REFERENCES"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "REFERENCES"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "**Wenjun Wu** received the PhD degree in computer science from Beihang University, in 2001. He was employed with Argonne National Laboratory as a research scientist working on grid computing, cloud computing, media collaboration, etc., until 2012. He is currently employed with Beihang University as a professor. His research interests include crowdsourcing, machine learning, cloud computing, eScience, and cyber infrastructure.\n\n![Portrait of Qi Dong, a master student at the School of Science and Media Intelligence, Communication University of China, China. Her current research interests are computer vision, Digital Image Processing.](223d6acc237d0154d7618df0673bd257_img.jpg)\n\nPortrait of Qi Dong, a master student at the School of Science and Media Intelligence, Communication University of China, China. Her current research interests are computer vision, Digital Image Processing.",
      "chunk_index": "40-14",
      "location": {
        "page_number": 13
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "REFERENCES"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "REFERENCES"
      }
    },
    {
      "document_id": "4304f94a-b81f-47d4-8698-0f9d380ca5dd",
      "content": "**Qi Dong** is a master student at the School of Science and Media Intelligence, Communication University of China, China. Her current research interests are computer vision, Digital Image Processing.",
      "chunk_index": "40-15",
      "location": {
        "page_number": 13
      },
      "header_path": [
        {
          "level": "Header 1",
          "name": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment"
        },
        {
          "level": "Header 2",
          "name": "REFERENCES"
        }
      ],
      "metadata": {
        "Header 1": "HF-VTON: High-Fidelity Virtual Try-On via Consistent Geometric and Semantic Alignment",
        "Header 2": "REFERENCES"
      }
    }
  ]
}